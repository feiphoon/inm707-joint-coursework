datetime:20210407-003658
hyperparameters:num_env:12, num_episodes:100000, learning_rate:0.002, hidden_layers:(128, 128)
results:min_reward:27.2, max_reward:190.5, reward_variance:30.991985802784566, mean_reward:125.196, mean_actor_loss::0.06630132098345287, mean_critic_loss:1.1356368824775505, mean_entropy_loss::3.023880908489227, mean_overall_loss:1.066305758814828
wall_time:320.1059990169997
