datetime:20210408-075553
hyperparameters:num_env:6, num_episodes:100000, learning_rate:0.003, hidden_layers:(128, 128), value_coefficient:0.5
results:min_reward:28.5, max_reward:300.7, reward_variance:44.91358703109783, mean_reward:121.65, mean_actor_loss::0.09714626490275381, mean_critic_loss:1.3156058892310223, mean_entropy_loss::3.0540558457374574, mean_overall_loss:1.215432526524592
wall_time:238.37139000000025
