{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.5 64-bit",
   "metadata": {
    "interpreter": {
     "hash": "4c4935044420ca1ae9e83f546a5afffe782132b550af3e301ea7a164f7ec9c09"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# Log processing"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from smart_open import open as smart_open\n",
    "import pandas as pd"
   ]
  },
  {
   "source": [
    "## Parse results log file"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Please change these before you start, otherwise you could overwrite something\n",
    "\n",
    "# LOG_FILE_PATH = \"results/CartPole-v1/train-a2c/20210405-225734/all_results.txt\"\n",
    "# RESULTS_CSV = \"training_results_20210405-225734.csv\"\n",
    "\n",
    "LOG_FILE_PATH = \"results/CartPole-v0/train-a2c/20210407-003658/all_results.txt\"\n",
    "RESULTS_CSV = \"training_results_20210407-003658.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "54"
      ]
     },
     "metadata": {},
     "execution_count": 18
    }
   ],
   "source": [
    "RESULTS = []\n",
    "SEPARATOR = \"==\"\n",
    "\n",
    "with smart_open(LOG_FILE_PATH, \"r\", encoding=\"utf-8\", errors=\"ignore\") as f:\n",
    "    contents = f.readlines()\n",
    "\n",
    "    # Strip unnecessary/empty new lines\n",
    "    contents = [line for line in contents if line != \"\\n\"]\n",
    "    contents = [line for line in contents if not line.startswith(SEPARATOR)]\n",
    "\n",
    "    enumerated_contents = list(enumerate(contents, 1))\n",
    "\n",
    "    for _ in range(0, len(enumerated_contents)-1, 4):\n",
    "        result = {}\n",
    "        result[\"datetime\"] = enumerated_contents[_][1].split(\":\")[1]\n",
    "        result[\"hyperparameters\"] = enumerated_contents[_ + 1][1].split(\":\", 1)[1]\n",
    "        result[\"results\"] = enumerated_contents[_ + 2][1].split(\":\", 1)[1]\n",
    "        result[\"wall_time\"] = enumerated_contents[_ + 3][1].split(\":\")[1]\n",
    "\n",
    "        RESULTS.append(result)        \n",
    "\n",
    "len(RESULTS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "            datetime                                    hyperparameters  \\\n",
       "0  20210407-003658\\n  num_env:6, num_episodes:50000, learning_rate:0...   \n",
       "1  20210407-003658\\n  num_env:6, num_episodes:50000, learning_rate:0...   \n",
       "2  20210407-003658\\n  num_env:6, num_episodes:50000, learning_rate:0...   \n",
       "3  20210407-003658\\n  num_env:6, num_episodes:50000, learning_rate:0...   \n",
       "4  20210407-003658\\n  num_env:6, num_episodes:50000, learning_rate:0...   \n",
       "\n",
       "                                             results             wall_time  \n",
       "0  min_reward:14.9, max_reward:119.6, reward_vari...  127.93577520200003\\n  \n",
       "1  min_reward:16.5, max_reward:148.4, reward_vari...  125.21537914199996\\n  \n",
       "2  min_reward:15.4, max_reward:142.1, reward_vari...       128.042971476\\n  \n",
       "3  min_reward:18.3, max_reward:144.8, reward_vari...  126.46709818199997\\n  \n",
       "4  min_reward:14.8, max_reward:171.7, reward_vari...  127.82372352799996\\n  "
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>datetime</th>\n      <th>hyperparameters</th>\n      <th>results</th>\n      <th>wall_time</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>20210407-003658\\n</td>\n      <td>num_env:6, num_episodes:50000, learning_rate:0...</td>\n      <td>min_reward:14.9, max_reward:119.6, reward_vari...</td>\n      <td>127.93577520200003\\n</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>20210407-003658\\n</td>\n      <td>num_env:6, num_episodes:50000, learning_rate:0...</td>\n      <td>min_reward:16.5, max_reward:148.4, reward_vari...</td>\n      <td>125.21537914199996\\n</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>20210407-003658\\n</td>\n      <td>num_env:6, num_episodes:50000, learning_rate:0...</td>\n      <td>min_reward:15.4, max_reward:142.1, reward_vari...</td>\n      <td>128.042971476\\n</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>20210407-003658\\n</td>\n      <td>num_env:6, num_episodes:50000, learning_rate:0...</td>\n      <td>min_reward:18.3, max_reward:144.8, reward_vari...</td>\n      <td>126.46709818199997\\n</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>20210407-003658\\n</td>\n      <td>num_env:6, num_episodes:50000, learning_rate:0...</td>\n      <td>min_reward:14.8, max_reward:171.7, reward_vari...</td>\n      <td>127.82372352799996\\n</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 19
    }
   ],
   "source": [
    "results_df = pd.DataFrame.from_dict(RESULTS)\n",
    "results_df.head(5)"
   ]
  },
  {
   "source": [
    "## Data cleaning"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Strip new lines\n",
    "results_df[\"datetime\"] = results_df[\"datetime\"].str.strip()\n",
    "results_df[\"hyperparameters\"] = results_df[\"hyperparameters\"].str.strip()\n",
    "results_df[\"results\"] = results_df[\"results\"].str.strip()\n",
    "results_df[\"results\"] = results_df[\"results\"].str.replace(\"::\", \":-\") # This is to fix a bad replace bug during logging - negative numbers must be preserved\n",
    "results_df[\"wall_time\"] = results_df[\"wall_time\"].str.strip()\n",
    "# results_df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split items in hyperparameters and results columns into lists\n",
    "results_df[\"hyperparameters\"] = results_df[\"hyperparameters\"].str.split(\",\", 3)\n",
    "results_df[\"results\"] = results_df[\"results\"].str.split(\",\")\n",
    "# results_df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make reliable index column\n",
    "results_df.reset_index(level=0, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Flatten hyperparameters and results columns\n",
    "flattened_hyperparameters_df = pd.DataFrame(results_df[\"hyperparameters\"].to_list(), columns=[\"num_env\", \"num_episodes\", \"learning_rate\", \"hidden_layers\"], index=results_df[\"index\"])\n",
    "flattened_results_df = pd.DataFrame(results_df[\"results\"].to_list(), columns=[\"min_reward\", \"max_reward\", \"reward_variance\", \"mean_reward\", \"mean_actor_loss\", \"mean_critic_loss\", \"mean_entropy_loss\", \"mean_overall_loss\"], index=results_df[\"index\"])\n",
    "flattened_hyperparameters_df.reset_index(level=0, inplace=True)\n",
    "flattened_results_df.reset_index(level=0, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "   index_p         datetime  \\\n",
       "0        0  20210407-003658   \n",
       "1        1  20210407-003658   \n",
       "2        2  20210407-003658   \n",
       "\n",
       "                                     hyperparameters  \\\n",
       "0  [num_env:6,  num_episodes:50000,  learning_rat...   \n",
       "1  [num_env:6,  num_episodes:50000,  learning_rat...   \n",
       "2  [num_env:6,  num_episodes:50000,  learning_rat...   \n",
       "\n",
       "                                             results           wall_time  \\\n",
       "0  [min_reward:14.9,  max_reward:119.6,  reward_v...  127.93577520200003   \n",
       "1  [min_reward:16.5,  max_reward:148.4,  reward_v...  125.21537914199996   \n",
       "2  [min_reward:15.4,  max_reward:142.1,  reward_v...       128.042971476   \n",
       "\n",
       "   index_r    num_env         num_episodes         learning_rate  \\\n",
       "0        0  num_env:6   num_episodes:50000   learning_rate:0.001   \n",
       "1        1  num_env:6   num_episodes:50000   learning_rate:0.001   \n",
       "2        2  num_env:6   num_episodes:50000   learning_rate:0.001   \n",
       "\n",
       "               hidden_layers  index       min_reward         max_reward  \\\n",
       "0     hidden_layers:(32, 32)      0  min_reward:14.9   max_reward:119.6   \n",
       "1     hidden_layers:(64, 64)      1  min_reward:16.5   max_reward:148.4   \n",
       "2   hidden_layers:(128, 128)      2  min_reward:15.4   max_reward:142.1   \n",
       "\n",
       "                       reward_variance                     mean_reward  \\\n",
       "0   reward_variance:25.978005235198488   mean_reward:72.66199999999999   \n",
       "1    reward_variance:30.42119583448356              mean_reward:90.462   \n",
       "2    reward_variance:27.70934687068607              mean_reward:98.764   \n",
       "\n",
       "                         mean_actor_loss  \\\n",
       "0   mean_actor_loss:-0.10722807213896826   \n",
       "1   mean_actor_loss:-0.10751617150514116   \n",
       "2    mean_actor_loss:-0.1073100725279418   \n",
       "\n",
       "                       mean_critic_loss  \\\n",
       "0     mean_critic_loss:2.42104439533448   \n",
       "1    mean_critic_loss:2.086534028534334   \n",
       "2   mean_critic_loss:1.8928728663544505   \n",
       "\n",
       "                        mean_entropy_loss  \\\n",
       "0     mean_entropy_loss:-3.22899555683136   \n",
       "1     mean_entropy_loss:-3.17665771484375   \n",
       "2   mean_entropy_loss:-3.1673620653152468   \n",
       "\n",
       "                       mean_overall_loss  \n",
       "0   mean_overall_loss:2.3106003895982283  \n",
       "1   mean_overall_loss:1.9758377702338388  \n",
       "2   mean_overall_loss:1.7824184411410824  "
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>index_p</th>\n      <th>datetime</th>\n      <th>hyperparameters</th>\n      <th>results</th>\n      <th>wall_time</th>\n      <th>index_r</th>\n      <th>num_env</th>\n      <th>num_episodes</th>\n      <th>learning_rate</th>\n      <th>hidden_layers</th>\n      <th>index</th>\n      <th>min_reward</th>\n      <th>max_reward</th>\n      <th>reward_variance</th>\n      <th>mean_reward</th>\n      <th>mean_actor_loss</th>\n      <th>mean_critic_loss</th>\n      <th>mean_entropy_loss</th>\n      <th>mean_overall_loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0</td>\n      <td>20210407-003658</td>\n      <td>[num_env:6,  num_episodes:50000,  learning_rat...</td>\n      <td>[min_reward:14.9,  max_reward:119.6,  reward_v...</td>\n      <td>127.93577520200003</td>\n      <td>0</td>\n      <td>num_env:6</td>\n      <td>num_episodes:50000</td>\n      <td>learning_rate:0.001</td>\n      <td>hidden_layers:(32, 32)</td>\n      <td>0</td>\n      <td>min_reward:14.9</td>\n      <td>max_reward:119.6</td>\n      <td>reward_variance:25.978005235198488</td>\n      <td>mean_reward:72.66199999999999</td>\n      <td>mean_actor_loss:-0.10722807213896826</td>\n      <td>mean_critic_loss:2.42104439533448</td>\n      <td>mean_entropy_loss:-3.22899555683136</td>\n      <td>mean_overall_loss:2.3106003895982283</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1</td>\n      <td>20210407-003658</td>\n      <td>[num_env:6,  num_episodes:50000,  learning_rat...</td>\n      <td>[min_reward:16.5,  max_reward:148.4,  reward_v...</td>\n      <td>125.21537914199996</td>\n      <td>1</td>\n      <td>num_env:6</td>\n      <td>num_episodes:50000</td>\n      <td>learning_rate:0.001</td>\n      <td>hidden_layers:(64, 64)</td>\n      <td>1</td>\n      <td>min_reward:16.5</td>\n      <td>max_reward:148.4</td>\n      <td>reward_variance:30.42119583448356</td>\n      <td>mean_reward:90.462</td>\n      <td>mean_actor_loss:-0.10751617150514116</td>\n      <td>mean_critic_loss:2.086534028534334</td>\n      <td>mean_entropy_loss:-3.17665771484375</td>\n      <td>mean_overall_loss:1.9758377702338388</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2</td>\n      <td>20210407-003658</td>\n      <td>[num_env:6,  num_episodes:50000,  learning_rat...</td>\n      <td>[min_reward:15.4,  max_reward:142.1,  reward_v...</td>\n      <td>128.042971476</td>\n      <td>2</td>\n      <td>num_env:6</td>\n      <td>num_episodes:50000</td>\n      <td>learning_rate:0.001</td>\n      <td>hidden_layers:(128, 128)</td>\n      <td>2</td>\n      <td>min_reward:15.4</td>\n      <td>max_reward:142.1</td>\n      <td>reward_variance:27.70934687068607</td>\n      <td>mean_reward:98.764</td>\n      <td>mean_actor_loss:-0.1073100725279418</td>\n      <td>mean_critic_loss:1.8928728663544505</td>\n      <td>mean_entropy_loss:-3.1673620653152468</td>\n      <td>mean_overall_loss:1.7824184411410824</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 24
    }
   ],
   "source": [
    "# Join new hyperparameters and results dfs to main df\n",
    "preprocessed_results_df = results_df.join(flattened_hyperparameters_df, on=\"index\", lsuffix=\"_p\")\n",
    "preprocessed_results_df = preprocessed_results_df.join(flattened_results_df, on=\"index\", lsuffix=\"_r\")\n",
    "preprocessed_results_df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "          datetime           wall_time    num_env         num_episodes  \\\n",
       "0  20210407-003658  127.93577520200003  num_env:6   num_episodes:50000   \n",
       "1  20210407-003658  125.21537914199996  num_env:6   num_episodes:50000   \n",
       "\n",
       "          learning_rate            hidden_layers  index       min_reward  \\\n",
       "0   learning_rate:0.001   hidden_layers:(32, 32)      0  min_reward:14.9   \n",
       "1   learning_rate:0.001   hidden_layers:(64, 64)      1  min_reward:16.5   \n",
       "\n",
       "          max_reward                      reward_variance  \\\n",
       "0   max_reward:119.6   reward_variance:25.978005235198488   \n",
       "1   max_reward:148.4    reward_variance:30.42119583448356   \n",
       "\n",
       "                      mean_reward                        mean_actor_loss  \\\n",
       "0   mean_reward:72.66199999999999   mean_actor_loss:-0.10722807213896826   \n",
       "1              mean_reward:90.462   mean_actor_loss:-0.10751617150514116   \n",
       "\n",
       "                      mean_critic_loss                     mean_entropy_loss  \\\n",
       "0    mean_critic_loss:2.42104439533448   mean_entropy_loss:-3.22899555683136   \n",
       "1   mean_critic_loss:2.086534028534334   mean_entropy_loss:-3.17665771484375   \n",
       "\n",
       "                       mean_overall_loss  \n",
       "0   mean_overall_loss:2.3106003895982283  \n",
       "1   mean_overall_loss:1.9758377702338388  "
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>datetime</th>\n      <th>wall_time</th>\n      <th>num_env</th>\n      <th>num_episodes</th>\n      <th>learning_rate</th>\n      <th>hidden_layers</th>\n      <th>index</th>\n      <th>min_reward</th>\n      <th>max_reward</th>\n      <th>reward_variance</th>\n      <th>mean_reward</th>\n      <th>mean_actor_loss</th>\n      <th>mean_critic_loss</th>\n      <th>mean_entropy_loss</th>\n      <th>mean_overall_loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>20210407-003658</td>\n      <td>127.93577520200003</td>\n      <td>num_env:6</td>\n      <td>num_episodes:50000</td>\n      <td>learning_rate:0.001</td>\n      <td>hidden_layers:(32, 32)</td>\n      <td>0</td>\n      <td>min_reward:14.9</td>\n      <td>max_reward:119.6</td>\n      <td>reward_variance:25.978005235198488</td>\n      <td>mean_reward:72.66199999999999</td>\n      <td>mean_actor_loss:-0.10722807213896826</td>\n      <td>mean_critic_loss:2.42104439533448</td>\n      <td>mean_entropy_loss:-3.22899555683136</td>\n      <td>mean_overall_loss:2.3106003895982283</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>20210407-003658</td>\n      <td>125.21537914199996</td>\n      <td>num_env:6</td>\n      <td>num_episodes:50000</td>\n      <td>learning_rate:0.001</td>\n      <td>hidden_layers:(64, 64)</td>\n      <td>1</td>\n      <td>min_reward:16.5</td>\n      <td>max_reward:148.4</td>\n      <td>reward_variance:30.42119583448356</td>\n      <td>mean_reward:90.462</td>\n      <td>mean_actor_loss:-0.10751617150514116</td>\n      <td>mean_critic_loss:2.086534028534334</td>\n      <td>mean_entropy_loss:-3.17665771484375</td>\n      <td>mean_overall_loss:1.9758377702338388</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 25
    }
   ],
   "source": [
    "# Drop redundant columns\n",
    "preprocessed_results_df = preprocessed_results_df.drop(columns=[\"index_r\", \"index_p\", \"hyperparameters\", \"results\"])\n",
    "preprocessed_results_df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "   index         datetime           wall_time    num_env         num_episodes  \\\n",
       "0      0  20210407-003658  127.93577520200003  num_env:6   num_episodes:50000   \n",
       "\n",
       "          learning_rate            hidden_layers       min_reward  \\\n",
       "0   learning_rate:0.001   hidden_layers:(32, 32)  min_reward:14.9   \n",
       "\n",
       "          max_reward                     mean_reward  \\\n",
       "0   max_reward:119.6   mean_reward:72.66199999999999   \n",
       "\n",
       "                       reward_variance                        mean_actor_loss  \\\n",
       "0   reward_variance:25.978005235198488   mean_actor_loss:-0.10722807213896826   \n",
       "\n",
       "                     mean_critic_loss                     mean_entropy_loss  \\\n",
       "0   mean_critic_loss:2.42104439533448   mean_entropy_loss:-3.22899555683136   \n",
       "\n",
       "                       mean_overall_loss  \n",
       "0   mean_overall_loss:2.3106003895982283  "
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>index</th>\n      <th>datetime</th>\n      <th>wall_time</th>\n      <th>num_env</th>\n      <th>num_episodes</th>\n      <th>learning_rate</th>\n      <th>hidden_layers</th>\n      <th>min_reward</th>\n      <th>max_reward</th>\n      <th>mean_reward</th>\n      <th>reward_variance</th>\n      <th>mean_actor_loss</th>\n      <th>mean_critic_loss</th>\n      <th>mean_entropy_loss</th>\n      <th>mean_overall_loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0</td>\n      <td>20210407-003658</td>\n      <td>127.93577520200003</td>\n      <td>num_env:6</td>\n      <td>num_episodes:50000</td>\n      <td>learning_rate:0.001</td>\n      <td>hidden_layers:(32, 32)</td>\n      <td>min_reward:14.9</td>\n      <td>max_reward:119.6</td>\n      <td>mean_reward:72.66199999999999</td>\n      <td>reward_variance:25.978005235198488</td>\n      <td>mean_actor_loss:-0.10722807213896826</td>\n      <td>mean_critic_loss:2.42104439533448</td>\n      <td>mean_entropy_loss:-3.22899555683136</td>\n      <td>mean_overall_loss:2.3106003895982283</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 26
    }
   ],
   "source": [
    "# Rearrange columns for better readability\n",
    "_columns = [\"index\", \"datetime\", \"wall_time\", \"num_env\", \"num_episodes\", \"learning_rate\", \"hidden_layers\", \"min_reward\", \"max_reward\", \"mean_reward\", \"reward_variance\", \"mean_actor_loss\", \"mean_critic_loss\", \"mean_entropy_loss\", \"mean_overall_loss\"]\n",
    "\n",
    "preprocessed_results_df = preprocessed_results_df[_columns]\n",
    "preprocessed_results_df.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(54, 15)"
      ]
     },
     "metadata": {},
     "execution_count": 27
    }
   ],
   "source": [
    "preprocessed_results_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "   index         datetime           wall_time num_env num_episodes  \\\n",
       "0      0  20210407-003658  127.93577520200003       6        50000   \n",
       "1      1  20210407-003658  125.21537914199996       6        50000   \n",
       "2      2  20210407-003658       128.042971476       6        50000   \n",
       "\n",
       "  learning_rate hidden_layers min_reward max_reward        mean_reward  \\\n",
       "0         0.001      (32, 32)       14.9      119.6  72.66199999999999   \n",
       "1         0.001      (64, 64)       16.5      148.4             90.462   \n",
       "2         0.001    (128, 128)       15.4      142.1             98.764   \n",
       "\n",
       "      reward_variance       mean_actor_loss    mean_critic_loss  \\\n",
       "0  25.978005235198488  -0.10722807213896826    2.42104439533448   \n",
       "1   30.42119583448356  -0.10751617150514116   2.086534028534334   \n",
       "2   27.70934687068607   -0.1073100725279418  1.8928728663544505   \n",
       "\n",
       "     mean_entropy_loss   mean_overall_loss  \n",
       "0    -3.22899555683136  2.3106003895982283  \n",
       "1    -3.17665771484375  1.9758377702338388  \n",
       "2  -3.1673620653152468  1.7824184411410824  "
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>index</th>\n      <th>datetime</th>\n      <th>wall_time</th>\n      <th>num_env</th>\n      <th>num_episodes</th>\n      <th>learning_rate</th>\n      <th>hidden_layers</th>\n      <th>min_reward</th>\n      <th>max_reward</th>\n      <th>mean_reward</th>\n      <th>reward_variance</th>\n      <th>mean_actor_loss</th>\n      <th>mean_critic_loss</th>\n      <th>mean_entropy_loss</th>\n      <th>mean_overall_loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0</td>\n      <td>20210407-003658</td>\n      <td>127.93577520200003</td>\n      <td>6</td>\n      <td>50000</td>\n      <td>0.001</td>\n      <td>(32, 32)</td>\n      <td>14.9</td>\n      <td>119.6</td>\n      <td>72.66199999999999</td>\n      <td>25.978005235198488</td>\n      <td>-0.10722807213896826</td>\n      <td>2.42104439533448</td>\n      <td>-3.22899555683136</td>\n      <td>2.3106003895982283</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1</td>\n      <td>20210407-003658</td>\n      <td>125.21537914199996</td>\n      <td>6</td>\n      <td>50000</td>\n      <td>0.001</td>\n      <td>(64, 64)</td>\n      <td>16.5</td>\n      <td>148.4</td>\n      <td>90.462</td>\n      <td>30.42119583448356</td>\n      <td>-0.10751617150514116</td>\n      <td>2.086534028534334</td>\n      <td>-3.17665771484375</td>\n      <td>1.9758377702338388</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2</td>\n      <td>20210407-003658</td>\n      <td>128.042971476</td>\n      <td>6</td>\n      <td>50000</td>\n      <td>0.001</td>\n      <td>(128, 128)</td>\n      <td>15.4</td>\n      <td>142.1</td>\n      <td>98.764</td>\n      <td>27.70934687068607</td>\n      <td>-0.1073100725279418</td>\n      <td>1.8928728663544505</td>\n      <td>-3.1673620653152468</td>\n      <td>1.7824184411410824</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 28
    }
   ],
   "source": [
    "# Remove text from each column\n",
    "preprocessed_results_df[\"min_reward\"] = preprocessed_results_df[\"min_reward\"].str.split(\":\").str[1]\n",
    "preprocessed_results_df[\"max_reward\"] = preprocessed_results_df[\"max_reward\"].str.split(\":\").str[1]\n",
    "preprocessed_results_df[\"mean_reward\"] = preprocessed_results_df[\"mean_reward\"].str.split(\":\").str[1]\n",
    "preprocessed_results_df[\"reward_variance\"] = preprocessed_results_df[\"reward_variance\"].str.split(\":\").str[1]\n",
    "preprocessed_results_df[\"mean_actor_loss\"] = preprocessed_results_df[\"mean_actor_loss\"].str.split(\":\").str[1]\n",
    "preprocessed_results_df[\"mean_critic_loss\"] = preprocessed_results_df[\"mean_critic_loss\"].str.split(\":\").str[1]\n",
    "preprocessed_results_df[\"mean_entropy_loss\"] = preprocessed_results_df[\"mean_entropy_loss\"].str.split(\":\").str[1]\n",
    "preprocessed_results_df[\"mean_overall_loss\"] = preprocessed_results_df[\"mean_overall_loss\"].str.split(\":\").str[1]\n",
    "\n",
    "preprocessed_results_df[\"num_env\"] = preprocessed_results_df[\"num_env\"].str.split(\":\").str[1]\n",
    "preprocessed_results_df[\"num_episodes\"] = preprocessed_results_df[\"num_episodes\"].str.split(\":\").str[1]\n",
    "preprocessed_results_df[\"learning_rate\"] = preprocessed_results_df[\"learning_rate\"].str.split(\":\").str[1]\n",
    "preprocessed_results_df[\"hidden_layers\"] = preprocessed_results_df[\"hidden_layers\"].str.split(\":\").str[1]\n",
    "\n",
    "preprocessed_results_df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save results to CSV\n",
    "preprocessed_results_df.to_csv(RESULTS_CSV, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}