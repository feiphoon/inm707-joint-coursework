{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.5 64-bit",
   "metadata": {
    "interpreter": {
     "hash": "4c4935044420ca1ae9e83f546a5afffe782132b550af3e301ea7a164f7ec9c09"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# Actor-Critic spike\n",
    "\n",
    "Plan:\n",
    "- Spike Actor-Critic\n",
    "- Quick test on a Gym env\n",
    "- Update requirements.txt"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Imports & setup"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### Essential tools"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generic setup\n",
    "from typing import Tuple"
   ]
  },
  {
   "source": [
    "### Examine Gym environments"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "from gym import envs\n",
    "print(envs.registry.all())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[0]"
      ]
     },
     "metadata": {},
     "execution_count": 28
    }
   ],
   "source": [
    "import gym\n",
    "env = gym.make(\"CartPole-v1\")\n",
    "\n",
    "# Reproducible gym environments\n",
    "env.seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(200,\n",
       " 195.0,\n",
       " Discrete(2),\n",
       " Box(-3.4028234663852886e+38, 3.4028234663852886e+38, (4,), float32))"
      ]
     },
     "metadata": {},
     "execution_count": 22
    }
   ],
   "source": [
    "# Check environment details\n",
    "# CartPole-v0 is 200, 195.0\n",
    "# CartPole-v1 is 500, 475.0\n",
    "env.spec.max_episode_steps, env.spec.reward_threshold, env.action_space, env.observation_space"
   ]
  },
  {
   "source": [
    "### Import PyTorch"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x11e708090>"
      ]
     },
     "metadata": {},
     "execution_count": 37
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch import distributions\n",
    "\n",
    "# Check for CUDA\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Reproducible results\n",
    "torch.manual_seed(0)"
   ]
  },
  {
   "source": [
    "## Actor-Critic base class"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ActorCritic(nn.Module):\n",
    "    \"\"\"\n",
    "    A base Actor-Critic class\n",
    "    \"\"\"\n",
    "    def __init__(self, num_inputs: int, num_outputs: int, hidden_layer_config: tuple = (10, 10), activation_func=F.relu) -> None:\n",
    "        \"\"\"\n",
    "        Note:\n",
    "        - num_inputs = size of the observation space\n",
    "        - num_outputs = no. of possible actions\n",
    "        - activation_func = activation function to use between layers. Using F.* format here.\n",
    "        https://discuss.pytorch.org/t/whats-the-difference-between-nn-relu-vs-f-relu/27599\n",
    "        \"\"\"\n",
    "        super(ActorCritic, self).__init__()\n",
    "\n",
    "        self.num_inputs: int = num_inputs\n",
    "        self.num_outputs: int = num_outputs\n",
    "        # https://deepai.org/machine-learning-glossary-and-terms/hidden-layer-machine-learning\n",
    "        self.hidden_layer_config: tuple = hidden_layer_config\n",
    "        self.activation_func = activation_func\n",
    "\n",
    "        # https://towardsdatascience.com/pytorch-layer-dimensions-what-sizes-should-they-be-and-why-4265a41e01fd\n",
    "        # Format of nn:\n",
    "        # https://pytorch.org/docs/stable/generated/torch.nn.Module.html\n",
    "\n",
    "        self.hidden_layers = nn.ModuleList(\n",
    "            [nn.Linear(hidden_layer_config[_], hidden_layer_config[_+1]) for _ in range(len(hidden_layer_config)-1)]\n",
    "        )\n",
    "\n",
    "        # self.input_layer = nn.Linear(\n",
    "        #     in_features=self.num_inputs,\n",
    "        #     out_features=self.hidden_layers[0],\n",
    "        #     bias=True\n",
    "        # )\n",
    "\n",
    "        # # Actor/probabilities\n",
    "        # # LogSoftmax - pg126 Deep Reinforcement Learning in Action\n",
    "        # self.policy_output_layer = nn.Sequential(\n",
    "        #     nn.Linear(\n",
    "        #         in_features=self.hidden_layers[-1],\n",
    "        #         out_features=self.num_outputs,\n",
    "        #         bias=True\n",
    "        #     ),\n",
    "        #     nn.LogSoftmax(dim=1)\n",
    "        # )\n",
    "\n",
    "        # Critic\n",
    "        # The output is expected to be a single number because \n",
    "        # it's an approximation of state value.\n",
    "        # https://stackoverflow.com/questions/55405961/why-does-sigmoid-function-outperform-tanh-and-softmax-in-this-case\n",
    "        # self.value_output_layer = nn.Linear(\n",
    "        #     in_features=self.hidden_layers[-1],\n",
    "        #     out_features=1,\n",
    "        #     bias=True\n",
    "        # )\n",
    "\n",
    "        # Softmax output: Actor needs to return probabilities for each available action\n",
    "        self.actor = nn.Sequential(\n",
    "            nn.Linear(self.num_inputs, 10),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(10, self.num_outputs),\n",
    "            nn.Softmax(dim=1)\n",
    "        )\n",
    "\n",
    "        self.critic = nn.Sequential(\n",
    "            nn.Linear(self.num_inputs, 10),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(10, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, state: torch.FloatTensor) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        # Assert this state is a Tensor of floats, or else\n",
    "        assert isinstance(state, torch.FloatTensor)\n",
    "\n",
    "        # x = self.activation_func(self.input_layer(state))\n",
    "\n",
    "        # for h in self.hidden_layers:\n",
    "        #     state = self.activation_func(h(x))\n",
    "        \n",
    "        # actor = self.policy_output_layer(x)\n",
    "        # critic = self.value_output_layer(x)\n",
    "\n",
    "        # https://www.kite.com/python/docs/torch.distributions.Categorical\n",
    "        # Example\n",
    "        # >>> m = Categorical(torch.tensor([ 0.01, 0.01, 0.97, 0.01 ]))\n",
    "        # >>> m.sample()  # heavily in favour of tensor(2)\n",
    "        action_probability_dist = distributions.Categorical(self.actor(state))\n",
    "        state_values = self.critic(state)\n",
    "        # state_values = critic\n",
    "        return action_probability_dist, state_values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "ActorCritic(\n  (hidden_layers): ModuleList(\n    (0): Linear(in_features=10, out_features=10, bias=True)\n  )\n  (actor): Sequential(\n    (0): Linear(in_features=4, out_features=10, bias=True)\n    (1): ReLU()\n    (2): Linear(in_features=10, out_features=2, bias=True)\n    (3): Softmax(dim=1)\n  )\n  (critic): Sequential(\n    (0): Linear(in_features=4, out_features=10, bias=True)\n    (1): ReLU()\n    (2): Linear(in_features=10, out_features=1, bias=True)\n  )\n)\n"
     ]
    }
   ],
   "source": [
    "# Parameters based on environment:\n",
    "NUM_OBSERVATIONS: int = env.observation_space.shape[0] # input\n",
    "NUM_ACTIONS: int = env.action_space.n # output\n",
    "# NUM_ACTIONS, NUM_OBSERVATIONS\n",
    "\n",
    "ac = ActorCritic(NUM_OBSERVATIONS, NUM_ACTIONS).to(device)\n",
    "print(ac)\n",
    "# print(vars(ac))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "20.0\n13.0\n45.0\n15.0\n10.0\n19.0\n52.0\n19.0\n47.0\n41.0\n"
     ]
    }
   ],
   "source": [
    "# One episode\n",
    "def do_one_episode(env: gym.Env):\n",
    "    state = env.reset()\n",
    "    done = False\n",
    "    total_reward = 0\n",
    "\n",
    "    while not done:\n",
    "        state = torch.unsqueeze(torch.FloatTensor(state), 0).to(device)\n",
    "        probability_dist, values = ac(state)\n",
    "        action_to_take = probability_dist.sample()\n",
    "        next_state, reward, done, _ = env.step(action_to_take.cpu().detach().numpy()[0])\n",
    "        state = next_state\n",
    "        total_reward += reward\n",
    "    return total_reward\n",
    "\n",
    "# Test for 10 episodes\n",
    "cartpole = gym.make(\"CartPole-v1\")\n",
    "for i in range(10):\n",
    "    reward = do_one_episode(cartpole)\n",
    "    print(reward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}