{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3",
   "language": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# Actor-Critic spike\n",
    "\n",
    "Plan:\n",
    "- Spike Actor-Critic\n",
    "- Quick test on a Gym env\n",
    "- Update requirements.txt"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Imports & setup"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### Essential tools"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generic setup\n",
    "# from typing import Tuple"
   ]
  },
  {
   "source": [
    "### Examine Gym environments"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "from gym import envs\n",
    "print(envs.registry.all())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[0]"
      ]
     },
     "metadata": {},
     "execution_count": 9
    }
   ],
   "source": [
    "import gym\n",
    "env = gym.make(\"CartPole-v1\")\n",
    "\n",
    "# Reproducible gym environments\n",
    "env.seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(500,\n",
       " 475.0,\n",
       " Discrete(2),\n",
       " Box(-3.4028234663852886e+38, 3.4028234663852886e+38, (4,), float32))"
      ]
     },
     "metadata": {},
     "execution_count": 10
    }
   ],
   "source": [
    "# Check environment details\n",
    "# CartPole-v0 is 200, 195.0\n",
    "# CartPole-v1 is 500, 475.0\n",
    "env.spec.max_episode_steps, env.spec.reward_threshold, env.action_space, env.observation_space"
   ]
  },
  {
   "source": [
    "### Import PyTorch"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x133a511f0>"
      ]
     },
     "metadata": {},
     "execution_count": 11
    }
   ],
   "source": [
    "import torch\n",
    "# import torch.nn as nn\n",
    "# import torch.nn.functional as F\n",
    "# from torch import distributions\n",
    "\n",
    "# Check for CUDA\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Reproducible results\n",
    "torch.manual_seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from actor_critic import ActorCritic"
   ]
  },
  {
   "source": [
    "## Actor-Critic base class"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Moved!\n",
    "# \n",
    "# # class ActorCritic(nn.Module):\n",
    "#     \"\"\"\n",
    "#     A base Actor-Critic class\n",
    "#     \"\"\"\n",
    "#     def __init__(self, num_inputs: int, num_outputs: int, hidden_layer_config: tuple = (10, 10), activation_func=F.relu) -> None:\n",
    "#         \"\"\"\n",
    "#         Note:\n",
    "#         - num_inputs = size of the observation space\n",
    "#         - num_outputs = no. of possible actions\n",
    "#         - activation_func = activation function to use between layers. Using F.* format here.\n",
    "#         https://discuss.pytorch.org/t/whats-the-difference-between-nn-relu-vs-f-relu/27599\n",
    "#         \"\"\"\n",
    "#         super(ActorCritic, self).__init__()\n",
    "\n",
    "#         self.num_inputs: int = num_inputs\n",
    "#         self.num_outputs: int = num_outputs\n",
    "#         # https://deepai.org/machine-learning-glossary-and-terms/hidden-layer-machine-learning\n",
    "#         self.hidden_layer_config: tuple = hidden_layer_config\n",
    "#         self.activation_func = activation_func\n",
    "\n",
    "#         # https://towardsdatascience.com/pytorch-layer-dimensions-what-sizes-should-they-be-and-why-4265a41e01fd\n",
    "#         # Format of nn:\n",
    "#         # https://pytorch.org/docs/stable/generated/torch.nn.Module.html\n",
    "\n",
    "#         self.hidden_layers = nn.ModuleList(\n",
    "#             [nn.Linear(hidden_layer_config[_], hidden_layer_config[_+1]) for _ in range(len(hidden_layer_config)-1)]\n",
    "#         )\n",
    "\n",
    "#         # self.input_layer = nn.Linear(\n",
    "#         #     in_features=self.num_inputs,\n",
    "#         #     out_features=self.hidden_layers[0],\n",
    "#         #     bias=True\n",
    "#         # )\n",
    "\n",
    "#         # # Actor/probabilities\n",
    "#         # # LogSoftmax - pg126 Deep Reinforcement Learning in Action\n",
    "#         # self.policy_output_layer = nn.Sequential(\n",
    "#         #     nn.Linear(\n",
    "#         #         in_features=self.hidden_layers[-1],\n",
    "#         #         out_features=self.num_outputs,\n",
    "#         #         bias=True\n",
    "#         #     ),\n",
    "#         #     nn.LogSoftmax(dim=1)\n",
    "#         # )\n",
    "\n",
    "#         # Critic\n",
    "#         # The output is expected to be a single number because \n",
    "#         # it's an approximation of state value.\n",
    "#         # https://stackoverflow.com/questions/55405961/why-does-sigmoid-function-outperform-tanh-and-softmax-in-this-case\n",
    "#         # self.value_output_layer = nn.Linear(\n",
    "#         #     in_features=self.hidden_layers[-1],\n",
    "#         #     out_features=1,\n",
    "#         #     bias=True\n",
    "#         # )\n",
    "\n",
    "#         # Softmax output: Actor needs to return probabilities for each available action\n",
    "#         self.actor = nn.Sequential(\n",
    "#             nn.Linear(self.num_inputs, 10),\n",
    "#             nn.ReLU(),\n",
    "#             nn.Linear(10, self.num_outputs),\n",
    "#             nn.Softmax(dim=1)\n",
    "#         )\n",
    "\n",
    "#         self.critic = nn.Sequential(\n",
    "#             nn.Linear(self.num_inputs, 10),\n",
    "#             nn.ReLU(),\n",
    "#             nn.Linear(10, 1)\n",
    "#         )\n",
    "\n",
    "#     def forward(self, state: torch.FloatTensor) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "#         # Assert this state is a Tensor of floats, or else\n",
    "#         assert isinstance(state, torch.FloatTensor)\n",
    "\n",
    "#         # x = self.activation_func(self.input_layer(state))\n",
    "\n",
    "#         # for h in self.hidden_layers:\n",
    "#         #     state = self.activation_func(h(x))\n",
    "        \n",
    "#         # actor = self.policy_output_layer(x)\n",
    "#         # critic = self.value_output_layer(x)\n",
    "\n",
    "#         # https://www.kite.com/python/docs/torch.distributions.Categorical\n",
    "#         # Example\n",
    "#         # >>> m = Categorical(torch.tensor([ 0.01, 0.01, 0.97, 0.01 ]))\n",
    "#         # >>> m.sample()  # heavily in favour of tensor(2)\n",
    "#         action_probability_dist = distributions.Categorical(self.actor(state))\n",
    "#         state_values = self.critic(state)\n",
    "#         # state_values = critic\n",
    "#         return action_probability_dist, state_values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "ActorCritic(\n  (hidden_layers): ModuleList(\n    (0): Linear(in_features=10, out_features=10, bias=True)\n  )\n  (actor): Sequential(\n    (0): Linear(in_features=4, out_features=10, bias=True)\n    (1): ReLU()\n    (2): Linear(in_features=10, out_features=2, bias=True)\n    (3): Softmax(dim=1)\n  )\n  (critic): Sequential(\n    (0): Linear(in_features=4, out_features=10, bias=True)\n    (1): ReLU()\n    (2): Linear(in_features=10, out_features=1, bias=True)\n  )\n)\n"
     ]
    }
   ],
   "source": [
    "# Parameters based on environment:\n",
    "NUM_OBSERVATIONS: int = env.observation_space.shape[0] # input\n",
    "NUM_ACTIONS: int = env.action_space.n # output\n",
    "# NUM_ACTIONS, NUM_OBSERVATIONS\n",
    "\n",
    "ac = ActorCritic(NUM_OBSERVATIONS, NUM_ACTIONS).to(device)\n",
    "print(ac)\n",
    "# print(vars(ac))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "0.1571]], grad_fn=<AddmmBackward>)\n",
      "13.0\n",
      "actor(state) tensor([[0.5076, 0.4924]], grad_fn=<SoftmaxBackward>)\n",
      "action_probability_dist Categorical(probs: torch.Size([1, 2]))\n",
      "action_probability_dist sampled 0\n",
      "state_values tensor([[-0.0462]], grad_fn=<AddmmBackward>)\n",
      "actor(state) tensor([[0.4990, 0.5010]], grad_fn=<SoftmaxBackward>)\n",
      "action_probability_dist Categorical(probs: torch.Size([1, 2]))\n",
      "action_probability_dist sampled 1\n",
      "state_values tensor([[-0.0704]], grad_fn=<AddmmBackward>)\n",
      "actor(state) tensor([[0.5076, 0.4924]], grad_fn=<SoftmaxBackward>)\n",
      "action_probability_dist Categorical(probs: torch.Size([1, 2]))\n",
      "action_probability_dist sampled 0\n",
      "state_values tensor([[-0.0416]], grad_fn=<AddmmBackward>)\n",
      "actor(state) tensor([[0.4990, 0.5010]], grad_fn=<SoftmaxBackward>)\n",
      "action_probability_dist Categorical(probs: torch.Size([1, 2]))\n",
      "action_probability_dist sampled 1\n",
      "state_values tensor([[-0.0684]], grad_fn=<AddmmBackward>)\n",
      "actor(state) tensor([[0.5076, 0.4924]], grad_fn=<SoftmaxBackward>)\n",
      "action_probability_dist Categorical(probs: torch.Size([1, 2]))\n",
      "action_probability_dist sampled 1\n",
      "state_values tensor([[-0.0373]], grad_fn=<AddmmBackward>)\n",
      "actor(state) tensor([[0.4990, 0.5010]], grad_fn=<SoftmaxBackward>)\n",
      "action_probability_dist Categorical(probs: torch.Size([1, 2]))\n",
      "action_probability_dist sampled 1\n",
      "state_values tensor([[-0.0666]], grad_fn=<AddmmBackward>)\n",
      "actor(state) tensor([[0.4903, 0.5097]], grad_fn=<SoftmaxBackward>)\n",
      "action_probability_dist Categorical(probs: torch.Size([1, 2]))\n",
      "action_probability_dist sampled 1\n",
      "state_values tensor([[-0.0886]], grad_fn=<AddmmBackward>)\n",
      "actor(state) tensor([[0.4987, 0.5013]], grad_fn=<SoftmaxBackward>)\n",
      "action_probability_dist Categorical(probs: torch.Size([1, 2]))\n",
      "action_probability_dist sampled 0\n",
      "state_values tensor([[-0.0638]], grad_fn=<AddmmBackward>)\n",
      "actor(state) tensor([[0.4892, 0.5108]], grad_fn=<SoftmaxBackward>)\n",
      "action_probability_dist Categorical(probs: torch.Size([1, 2]))\n",
      "action_probability_dist sampled 1\n",
      "state_values tensor([[-0.0870]], grad_fn=<AddmmBackward>)\n",
      "actor(state) tensor([[0.4982, 0.5018]], grad_fn=<SoftmaxBackward>)\n",
      "action_probability_dist Categorical(probs: torch.Size([1, 2]))\n",
      "action_probability_dist sampled 0\n",
      "state_values tensor([[-0.0616]], grad_fn=<AddmmBackward>)\n",
      "actor(state) tensor([[0.4874, 0.5126]], grad_fn=<SoftmaxBackward>)\n",
      "action_probability_dist Categorical(probs: torch.Size([1, 2]))\n",
      "action_probability_dist sampled 0\n",
      "state_values tensor([[-0.0851]], grad_fn=<AddmmBackward>)\n",
      "actor(state) tensor([[0.4978, 0.5022]], grad_fn=<SoftmaxBackward>)\n",
      "action_probability_dist Categorical(probs: torch.Size([1, 2]))\n",
      "action_probability_dist sampled 1\n",
      "state_values tensor([[-0.0600]], grad_fn=<AddmmBackward>)\n",
      "actor(state) tensor([[0.4857, 0.5143]], grad_fn=<SoftmaxBackward>)\n",
      "action_probability_dist Categorical(probs: torch.Size([1, 2]))\n",
      "action_probability_dist sampled 1\n",
      "state_values tensor([[-0.0838]], grad_fn=<AddmmBackward>)\n",
      "actor(state) tensor([[0.4972, 0.5028]], grad_fn=<SoftmaxBackward>)\n",
      "action_probability_dist Categorical(probs: torch.Size([1, 2]))\n",
      "action_probability_dist sampled 1\n",
      "state_values tensor([[-0.0590]], grad_fn=<AddmmBackward>)\n",
      "actor(state) tensor([[0.5057, 0.4943]], grad_fn=<SoftmaxBackward>)\n",
      "action_probability_dist Categorical(probs: torch.Size([1, 2]))\n",
      "action_probability_dist sampled 1\n",
      "state_values tensor([[-0.0244]], grad_fn=<AddmmBackward>)\n",
      "actor(state) tensor([[0.4970, 0.5030]], grad_fn=<SoftmaxBackward>)\n",
      "action_probability_dist Categorical(probs: torch.Size([1, 2]))\n",
      "action_probability_dist sampled 1\n",
      "state_values tensor([[-0.0598]], grad_fn=<AddmmBackward>)\n",
      "actor(state) tensor([[0.4829, 0.5171]], grad_fn=<SoftmaxBackward>)\n",
      "action_probability_dist Categorical(probs: torch.Size([1, 2]))\n",
      "action_probability_dist sampled 1\n",
      "state_values tensor([[-0.0840]], grad_fn=<AddmmBackward>)\n",
      "actor(state) tensor([[0.4710, 0.5290]], grad_fn=<SoftmaxBackward>)\n",
      "action_probability_dist Categorical(probs: torch.Size([1, 2]))\n",
      "action_probability_dist sampled 0\n",
      "state_values tensor([[-0.0672]], grad_fn=<AddmmBackward>)\n",
      "actor(state) tensor([[0.4584, 0.5416]], grad_fn=<SoftmaxBackward>)\n",
      "action_probability_dist Categorical(probs: torch.Size([1, 2]))\n",
      "action_probability_dist sampled 1\n",
      "state_values tensor([[-0.0656]], grad_fn=<AddmmBackward>)\n",
      "actor(state) tensor([[0.4452, 0.5548]], grad_fn=<SoftmaxBackward>)\n",
      "action_probability_dist Categorical(probs: torch.Size([1, 2]))\n",
      "action_probability_dist sampled 1\n",
      "state_values tensor([[-0.0633]], grad_fn=<AddmmBackward>)\n",
      "actor(state) tensor([[0.4313, 0.5687]], grad_fn=<SoftmaxBackward>)\n",
      "action_probability_dist Categorical(probs: torch.Size([1, 2]))\n",
      "action_probability_dist sampled 0\n",
      "state_values tensor([[-0.0579]], grad_fn=<AddmmBackward>)\n",
      "actor(state) tensor([[0.4168, 0.5832]], grad_fn=<SoftmaxBackward>)\n",
      "action_probability_dist Categorical(probs: torch.Size([1, 2]))\n",
      "action_probability_dist sampled 1\n",
      "state_values tensor([[-0.0369]], grad_fn=<AddmmBackward>)\n",
      "actor(state) tensor([[0.4218, 0.5782]], grad_fn=<SoftmaxBackward>)\n",
      "action_probability_dist Categorical(probs: torch.Size([1, 2]))\n",
      "action_probability_dist sampled 1\n",
      "state_values tensor([[-0.0396]], grad_fn=<AddmmBackward>)\n",
      "23.0\n",
      "actor(state) tensor([[0.5081, 0.4919]], grad_fn=<SoftmaxBackward>)\n",
      "action_probability_dist Categorical(probs: torch.Size([1, 2]))\n",
      "action_probability_dist sampled 0\n",
      "state_values tensor([[-0.0495]], grad_fn=<AddmmBackward>)\n",
      "actor(state) tensor([[0.5169, 0.4831]], grad_fn=<SoftmaxBackward>)\n",
      "action_probability_dist Categorical(probs: torch.Size([1, 2]))\n",
      "action_probability_dist sampled 1\n",
      "state_values tensor([[0.0067]], grad_fn=<AddmmBackward>)\n",
      "actor(state) tensor([[0.5244, 0.4756]], grad_fn=<SoftmaxBackward>)\n",
      "action_probability_dist Categorical(probs: torch.Size([1, 2]))\n",
      "action_probability_dist sampled 0\n",
      "state_values tensor([[0.0307]], grad_fn=<AddmmBackward>)\n",
      "actor(state) tensor([[0.5177, 0.4823]], grad_fn=<SoftmaxBackward>)\n",
      "action_probability_dist Categorical(probs: torch.Size([1, 2]))\n",
      "action_probability_dist sampled 0\n",
      "state_values tensor([[0.0066]], grad_fn=<AddmmBackward>)\n",
      "actor(state) tensor([[0.5093, 0.4907]], grad_fn=<SoftmaxBackward>)\n",
      "action_probability_dist Categorical(probs: torch.Size([1, 2]))\n",
      "action_probability_dist sampled 0\n",
      "state_values tensor([[-0.0444]], grad_fn=<AddmmBackward>)\n",
      "actor(state) tensor([[0.5182, 0.4818]], grad_fn=<SoftmaxBackward>)\n",
      "action_probability_dist Categorical(probs: torch.Size([1, 2]))\n",
      "action_probability_dist sampled 1\n",
      "state_values tensor([[0.0092]], grad_fn=<AddmmBackward>)\n",
      "actor(state) tensor([[0.5226, 0.4774]], grad_fn=<SoftmaxBackward>)\n",
      "action_probability_dist Categorical(probs: torch.Size([1, 2]))\n",
      "action_probability_dist sampled 0\n",
      "state_values tensor([[0.0339]], grad_fn=<AddmmBackward>)\n",
      "actor(state) tensor([[0.5179, 0.4821]], grad_fn=<SoftmaxBackward>)\n",
      "action_probability_dist Categorical(probs: torch.Size([1, 2]))\n",
      "action_probability_dist sampled 0\n",
      "state_values tensor([[0.0579]], grad_fn=<AddmmBackward>)\n",
      "actor(state) tensor([[0.5215, 0.4785]], grad_fn=<SoftmaxBackward>)\n",
      "action_probability_dist Categorical(probs: torch.Size([1, 2]))\n",
      "action_probability_dist sampled 0\n",
      "state_values tensor([[0.0341]], grad_fn=<AddmmBackward>)\n",
      "actor(state) tensor([[0.5207, 0.4793]], grad_fn=<SoftmaxBackward>)\n",
      "action_probability_dist Categorical(probs: torch.Size([1, 2]))\n",
      "action_probability_dist sampled 0\n",
      "state_values tensor([[0.0117]], grad_fn=<AddmmBackward>)\n",
      "actor(state) tensor([[0.5126, 0.4874]], grad_fn=<SoftmaxBackward>)\n",
      "action_probability_dist Categorical(probs: torch.Size([1, 2]))\n",
      "action_probability_dist sampled 0\n",
      "state_values tensor([[-0.0243]], grad_fn=<AddmmBackward>)\n",
      "actor(state) tensor([[0.5217, 0.4783]], grad_fn=<SoftmaxBackward>)\n",
      "action_probability_dist Categorical(probs: torch.Size([1, 2]))\n",
      "action_probability_dist sampled 1\n",
      "state_values tensor([[0.0177]], grad_fn=<AddmmBackward>)\n",
      "actor(state) tensor([[0.5138, 0.4862]], grad_fn=<SoftmaxBackward>)\n",
      "action_probability_dist Categorical(probs: torch.Size([1, 2]))\n",
      "action_probability_dist sampled 1\n",
      "state_values tensor([[-0.0114]], grad_fn=<AddmmBackward>)\n",
      "actor(state) tensor([[0.5057, 0.4943]], grad_fn=<SoftmaxBackward>)\n",
      "action_probability_dist Categorical(probs: torch.Size([1, 2]))\n",
      "action_probability_dist sampled 0\n",
      "state_values tensor([[-0.0532]], grad_fn=<AddmmBackward>)\n",
      "actor(state) tensor([[0.4974, 0.5026]], grad_fn=<SoftmaxBackward>)\n",
      "action_probability_dist Categorical(probs: torch.Size([1, 2]))\n",
      "action_probability_dist sampled 0\n",
      "state_values tensor([[-0.0808]], grad_fn=<AddmmBackward>)\n",
      "actor(state) tensor([[0.5064, 0.4936]], grad_fn=<SoftmaxBackward>)\n",
      "action_probability_dist Categorical(probs: torch.Size([1, 2]))\n",
      "action_probability_dist sampled 1\n",
      "state_values tensor([[-0.0426]], grad_fn=<AddmmBackward>)\n",
      "actor(state) tensor([[0.5155, 0.4845]], grad_fn=<SoftmaxBackward>)\n",
      "action_probability_dist Categorical(probs: torch.Size([1, 2]))\n",
      "action_probability_dist sampled 1\n",
      "state_values tensor([[0.0169]], grad_fn=<AddmmBackward>)\n",
      "actor(state) tensor([[0.5179, 0.4821]], grad_fn=<SoftmaxBackward>)\n",
      "action_probability_dist Categorical(probs: torch.Size([1, 2]))\n",
      "action_probability_dist sampled 0\n",
      "state_values tensor([[0.0444]], grad_fn=<AddmmBackward>)\n",
      "actor(state) tensor([[0.5171, 0.4829]], grad_fn=<SoftmaxBackward>)\n",
      "action_probability_dist Categorical(probs: torch.Size([1, 2]))\n",
      "action_probability_dist sampled 0\n",
      "state_values tensor([[0.0246]], grad_fn=<AddmmBackward>)\n",
      "actor(state) tensor([[0.5093, 0.4907]], grad_fn=<SoftmaxBackward>)\n",
      "action_probability_dist Categorical(probs: torch.Size([1, 2]))\n",
      "action_probability_dist sampled 0\n",
      "state_values tensor([[-0.0146]], grad_fn=<AddmmBackward>)\n",
      "actor(state) tensor([[0.5014, 0.4986]], grad_fn=<SoftmaxBackward>)\n",
      "action_probability_dist Categorical(probs: torch.Size([1, 2]))\n",
      "action_probability_dist sampled 1\n",
      "state_values tensor([[-0.0524]], grad_fn=<AddmmBackward>)\n",
      "actor(state) tensor([[0.5106, 0.4894]], grad_fn=<SoftmaxBackward>)\n",
      "action_probability_dist Categorical(probs: torch.Size([1, 2]))\n",
      "action_probability_dist sampled 0\n",
      "state_values tensor([[0.0070]], grad_fn=<AddmmBackward>)\n",
      "22.0\n",
      "actor(state) tensor([[0.5089, 0.4911]], grad_fn=<SoftmaxBackward>)\n",
      "action_probability_dist Categorical(probs: torch.Size([1, 2]))\n",
      "action_probability_dist sampled 1\n",
      "state_values tensor([[-0.0361]], grad_fn=<AddmmBackward>)\n",
      "actor(state) tensor([[0.5177, 0.4823]], grad_fn=<SoftmaxBackward>)\n",
      "action_probability_dist Categorical(probs: torch.Size([1, 2]))\n",
      "action_probability_dist sampled 0\n",
      "state_values tensor([[0.0185]], grad_fn=<AddmmBackward>)\n",
      "actor(state) tensor([[0.5094, 0.4906]], grad_fn=<SoftmaxBackward>)\n",
      "action_probability_dist Categorical(probs: torch.Size([1, 2]))\n",
      "action_probability_dist sampled 0\n",
      "state_values tensor([[-0.0322]], grad_fn=<AddmmBackward>)\n",
      "actor(state) tensor([[0.5182, 0.4818]], grad_fn=<SoftmaxBackward>)\n",
      "action_probability_dist Categorical(probs: torch.Size([1, 2]))\n",
      "action_probability_dist sampled 1\n",
      "state_values tensor([[0.0203]], grad_fn=<AddmmBackward>)\n",
      "actor(state) tensor([[0.5216, 0.4784]], grad_fn=<SoftmaxBackward>)\n",
      "action_probability_dist Categorical(probs: torch.Size([1, 2]))\n",
      "action_probability_dist sampled 1\n",
      "state_values tensor([[0.0446]], grad_fn=<AddmmBackward>)\n",
      "actor(state) tensor([[0.5192, 0.4808]], grad_fn=<SoftmaxBackward>)\n",
      "action_probability_dist Categorical(probs: torch.Size([1, 2]))\n",
      "action_probability_dist sampled 1\n",
      "state_values tensor([[0.0208]], grad_fn=<AddmmBackward>)\n",
      "actor(state) tensor([[0.5109, 0.4891]], grad_fn=<SoftmaxBackward>)\n",
      "action_probability_dist Categorical(probs: torch.Size([1, 2]))\n",
      "action_probability_dist sampled 1\n",
      "state_values tensor([[-0.0230]], grad_fn=<AddmmBackward>)\n",
      "actor(state) tensor([[0.5199, 0.4801]], grad_fn=<SoftmaxBackward>)\n",
      "action_probability_dist Categorical(probs: torch.Size([1, 2]))\n",
      "action_probability_dist sampled 0\n",
      "state_values tensor([[0.0242]], grad_fn=<AddmmBackward>)\n",
      "actor(state) tensor([[0.5118, 0.4882]], grad_fn=<SoftmaxBackward>)\n",
      "action_probability_dist Categorical(probs: torch.Size([1, 2]))\n",
      "action_probability_dist sampled 1\n",
      "state_values tensor([[-0.0151]], grad_fn=<AddmmBackward>)\n",
      "actor(state) tensor([[0.5035, 0.4965]], grad_fn=<SoftmaxBackward>)\n",
      "action_probability_dist Categorical(probs: torch.Size([1, 2]))\n",
      "action_probability_dist sampled 1\n",
      "state_values tensor([[-0.0556]], grad_fn=<AddmmBackward>)\n",
      "actor(state) tensor([[0.4950, 0.5050]], grad_fn=<SoftmaxBackward>)\n",
      "action_probability_dist Categorical(probs: torch.Size([1, 2]))\n",
      "action_probability_dist sampled 0\n",
      "state_values tensor([[-0.0780]], grad_fn=<AddmmBackward>)\n",
      "actor(state) tensor([[0.4864, 0.5136]], grad_fn=<SoftmaxBackward>)\n",
      "action_probability_dist Categorical(probs: torch.Size([1, 2]))\n",
      "action_probability_dist sampled 1\n",
      "state_values tensor([[-0.0853]], grad_fn=<AddmmBackward>)\n",
      "actor(state) tensor([[0.4797, 0.5203]], grad_fn=<SoftmaxBackward>)\n",
      "action_probability_dist Categorical(probs: torch.Size([1, 2]))\n",
      "action_probability_dist sampled 1\n",
      "state_values tensor([[-0.0853]], grad_fn=<AddmmBackward>)\n",
      "actor(state) tensor([[0.4860, 0.5140]], grad_fn=<SoftmaxBackward>)\n",
      "action_probability_dist Categorical(probs: torch.Size([1, 2]))\n",
      "action_probability_dist sampled 1\n",
      "state_values tensor([[-0.0843]], grad_fn=<AddmmBackward>)\n",
      "actor(state) tensor([[0.4945, 0.5055]], grad_fn=<SoftmaxBackward>)\n",
      "action_probability_dist Categorical(probs: torch.Size([1, 2]))\n",
      "action_probability_dist sampled 0\n",
      "state_values tensor([[-0.0639]], grad_fn=<AddmmBackward>)\n",
      "actor(state) tensor([[0.4859, 0.5141]], grad_fn=<SoftmaxBackward>)\n",
      "action_probability_dist Categorical(probs: torch.Size([1, 2]))\n",
      "action_probability_dist sampled 1\n",
      "state_values tensor([[-0.0839]], grad_fn=<AddmmBackward>)\n",
      "actor(state) tensor([[0.4944, 0.5056]], grad_fn=<SoftmaxBackward>)\n",
      "action_probability_dist Categorical(probs: torch.Size([1, 2]))\n",
      "action_probability_dist sampled 0\n",
      "state_values tensor([[-0.0587]], grad_fn=<AddmmBackward>)\n",
      "actor(state) tensor([[0.5031, 0.4969]], grad_fn=<SoftmaxBackward>)\n",
      "action_probability_dist Categorical(probs: torch.Size([1, 2]))\n",
      "action_probability_dist sampled 0\n",
      "state_values tensor([[-0.0248]], grad_fn=<AddmmBackward>)\n",
      "actor(state) tensor([[0.5120, 0.4880]], grad_fn=<SoftmaxBackward>)\n",
      "action_probability_dist Categorical(probs: torch.Size([1, 2]))\n",
      "action_probability_dist sampled 1\n",
      "state_values tensor([[0.0393]], grad_fn=<AddmmBackward>)\n",
      "actor(state) tensor([[0.5037, 0.4963]], grad_fn=<SoftmaxBackward>)\n",
      "action_probability_dist Categorical(probs: torch.Size([1, 2]))\n",
      "action_probability_dist sampled 1\n",
      "state_values tensor([[-0.0207]], grad_fn=<AddmmBackward>)\n",
      "actor(state) tensor([[0.5127, 0.4873]], grad_fn=<SoftmaxBackward>)\n",
      "action_probability_dist Categorical(probs: torch.Size([1, 2]))\n",
      "action_probability_dist sampled 1\n",
      "state_values tensor([[0.0429]], grad_fn=<AddmmBackward>)\n",
      "actor(state) tensor([[0.5167, 0.4833]], grad_fn=<SoftmaxBackward>)\n",
      "action_probability_dist Categorical(probs: torch.Size([1, 2]))\n",
      "action_probability_dist sampled 0\n",
      "state_values tensor([[0.0680]], grad_fn=<AddmmBackward>)\n",
      "actor(state) tensor([[0.5169, 0.4831]], grad_fn=<SoftmaxBackward>)\n",
      "action_probability_dist Categorical(probs: torch.Size([1, 2]))\n",
      "action_probability_dist sampled 1\n",
      "state_values tensor([[0.0925]], grad_fn=<AddmmBackward>)\n",
      "actor(state) tensor([[0.5171, 0.4829]], grad_fn=<SoftmaxBackward>)\n",
      "action_probability_dist Categorical(probs: torch.Size([1, 2]))\n",
      "action_probability_dist sampled 1\n",
      "state_values tensor([[0.1303]], grad_fn=<AddmmBackward>)\n",
      "actor(state) tensor([[0.5171, 0.4829]], grad_fn=<SoftmaxBackward>)\n",
      "action_probability_dist Categorical(probs: torch.Size([1, 2]))\n",
      "action_probability_dist sampled 1\n",
      "state_values tensor([[0.0953]], grad_fn=<AddmmBackward>)\n",
      "actor(state) tensor([[0.5173, 0.4827]], grad_fn=<SoftmaxBackward>)\n",
      "action_probability_dist Categorical(probs: torch.Size([1, 2]))\n",
      "action_probability_dist sampled 0\n",
      "state_values tensor([[0.1364]], grad_fn=<AddmmBackward>)\n",
      "actor(state) tensor([[0.5135, 0.4865]], grad_fn=<SoftmaxBackward>)\n",
      "action_probability_dist Categorical(probs: torch.Size([1, 2]))\n",
      "action_probability_dist sampled 0\n",
      "state_values tensor([[0.1776]], grad_fn=<AddmmBackward>)\n",
      "27.0\n",
      "actor(state) tensor([[0.5077, 0.4923]], grad_fn=<SoftmaxBackward>)\n",
      "action_probability_dist Categorical(probs: torch.Size([1, 2]))\n",
      "action_probability_dist sampled 0\n",
      "state_values tensor([[-0.0453]], grad_fn=<AddmmBackward>)\n",
      "actor(state) tensor([[0.5164, 0.4836]], grad_fn=<SoftmaxBackward>)\n",
      "action_probability_dist Categorical(probs: torch.Size([1, 2]))\n",
      "action_probability_dist sampled 0\n",
      "state_values tensor([[0.0077]], grad_fn=<AddmmBackward>)\n",
      "actor(state) tensor([[0.5246, 0.4754]], grad_fn=<SoftmaxBackward>)\n",
      "action_probability_dist Categorical(probs: torch.Size([1, 2]))\n",
      "action_probability_dist sampled 0\n",
      "state_values tensor([[0.0310]], grad_fn=<AddmmBackward>)\n",
      "actor(state) tensor([[0.5171, 0.4829]], grad_fn=<SoftmaxBackward>)\n",
      "action_probability_dist Categorical(probs: torch.Size([1, 2]))\n",
      "action_probability_dist sampled 0\n",
      "state_values tensor([[0.0061]], grad_fn=<AddmmBackward>)\n",
      "actor(state) tensor([[0.5087, 0.4913]], grad_fn=<SoftmaxBackward>)\n",
      "action_probability_dist Categorical(probs: torch.Size([1, 2]))\n",
      "action_probability_dist sampled 0\n",
      "state_values tensor([[-0.0438]], grad_fn=<AddmmBackward>)\n",
      "actor(state) tensor([[0.5175, 0.4825]], grad_fn=<SoftmaxBackward>)\n",
      "action_probability_dist Categorical(probs: torch.Size([1, 2]))\n",
      "action_probability_dist sampled 0\n",
      "state_values tensor([[0.0073]], grad_fn=<AddmmBackward>)\n",
      "actor(state) tensor([[0.5091, 0.4909]], grad_fn=<SoftmaxBackward>)\n",
      "action_probability_dist Categorical(probs: torch.Size([1, 2]))\n",
      "action_probability_dist sampled 1\n",
      "state_values tensor([[-0.0402]], grad_fn=<AddmmBackward>)\n",
      "actor(state) tensor([[0.5179, 0.4821]], grad_fn=<SoftmaxBackward>)\n",
      "action_probability_dist Categorical(probs: torch.Size([1, 2]))\n",
      "action_probability_dist sampled 1\n",
      "state_values tensor([[0.0088]], grad_fn=<AddmmBackward>)\n",
      "actor(state) tensor([[0.5096, 0.4904]], grad_fn=<SoftmaxBackward>)\n",
      "action_probability_dist Categorical(probs: torch.Size([1, 2]))\n",
      "action_probability_dist sampled 0\n",
      "state_values tensor([[-0.0359]], grad_fn=<AddmmBackward>)\n",
      "actor(state) tensor([[0.5185, 0.4815]], grad_fn=<SoftmaxBackward>)\n",
      "action_probability_dist Categorical(probs: torch.Size([1, 2]))\n",
      "action_probability_dist sampled 1\n",
      "state_values tensor([[0.0107]], grad_fn=<AddmmBackward>)\n",
      "actor(state) tensor([[0.5220, 0.4780]], grad_fn=<SoftmaxBackward>)\n",
      "action_probability_dist Categorical(probs: torch.Size([1, 2]))\n",
      "action_probability_dist sampled 1\n",
      "state_values tensor([[0.0351]], grad_fn=<AddmmBackward>)\n",
      "actor(state) tensor([[0.5195, 0.4805]], grad_fn=<SoftmaxBackward>)\n",
      "action_probability_dist Categorical(probs: torch.Size([1, 2]))\n",
      "action_probability_dist sampled 0\n",
      "state_values tensor([[0.0115]], grad_fn=<AddmmBackward>)\n",
      "actor(state) tensor([[0.5209, 0.4791]], grad_fn=<SoftmaxBackward>)\n",
      "action_probability_dist Categorical(probs: torch.Size([1, 2]))\n",
      "action_probability_dist sampled 1\n",
      "state_values tensor([[0.0365]], grad_fn=<AddmmBackward>)\n",
      "actor(state) tensor([[0.5207, 0.4793]], grad_fn=<SoftmaxBackward>)\n",
      "action_probability_dist Categorical(probs: torch.Size([1, 2]))\n",
      "action_probability_dist sampled 0\n",
      "state_values tensor([[0.0136]], grad_fn=<AddmmBackward>)\n",
      "actor(state) tensor([[0.5194, 0.4806]], grad_fn=<SoftmaxBackward>)\n",
      "action_probability_dist Categorical(probs: torch.Size([1, 2]))\n",
      "action_probability_dist sampled 0\n",
      "state_values tensor([[0.0392]], grad_fn=<AddmmBackward>)\n",
      "actor(state) tensor([[0.5180, 0.4820]], grad_fn=<SoftmaxBackward>)\n",
      "action_probability_dist Categorical(probs: torch.Size([1, 2]))\n",
      "action_probability_dist sampled 1\n",
      "state_values tensor([[0.0715]], grad_fn=<AddmmBackward>)\n",
      "actor(state) tensor([[0.5182, 0.4818]], grad_fn=<SoftmaxBackward>)\n",
      "action_probability_dist Categorical(probs: torch.Size([1, 2]))\n",
      "action_probability_dist sampled 0\n",
      "state_values tensor([[0.1121]], grad_fn=<AddmmBackward>)\n",
      "actor(state) tensor([[0.5183, 0.4817]], grad_fn=<SoftmaxBackward>)\n",
      "action_probability_dist Categorical(probs: torch.Size([1, 2]))\n",
      "action_probability_dist sampled 0\n",
      "state_values tensor([[0.0790]], grad_fn=<AddmmBackward>)\n",
      "18.0\n"
     ]
    }
   ],
   "source": [
    "# One episode\n",
    "def do_one_episode(env: gym.Env):\n",
    "    state = env.reset()\n",
    "    done = False\n",
    "    total_reward = 0\n",
    "\n",
    "    while not done:\n",
    "        state = torch.unsqueeze(torch.FloatTensor(state), 0).to(device)\n",
    "        probability_dist, values = ac(state)\n",
    "        action_to_take = probability_dist.sample()\n",
    "        next_state, reward, done, _ = env.step(action_to_take.cpu().detach().numpy()[0])\n",
    "        state = next_state\n",
    "        total_reward += reward\n",
    "    return total_reward\n",
    "\n",
    "# Test for 10 episodes\n",
    "cartpole = gym.make(\"CartPole-v1\")\n",
    "for i in range(10):\n",
    "    reward = do_one_episode(cartpole)\n",
    "    print(reward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Categorical(probs: torch.Size([4]))\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "tensor(1)"
      ]
     },
     "metadata": {},
     "execution_count": 16
    }
   ],
   "source": [
    "from torch import distributions\n",
    "m = distributions.Categorical(torch.tensor([ 0.25, 0.25, 0.25, 0.25 ]))\n",
    "print(m)\n",
    "m.sample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}