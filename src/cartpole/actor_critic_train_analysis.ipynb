{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3",
   "language": "python"
  },
  "metadata": {
   "interpreter": {
    "hash": "ec04c10a3ef611ea2836b5ff833f0501e0dcfc8e0500f516ccda9246622a0e8e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports for analysis\n",
    "from smart_open import open as smart_open\n",
    "import pandas as pd\n",
    "# import numpy as np\n",
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "pd.set_option('display.max_rows', 10)\n",
    "pd.set_option('display.max_columns', 500)\n",
    "pd.set_option('display.width', 150)"
   ]
  },
  {
   "source": [
    "## Parse results log file\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "54"
      ]
     },
     "metadata": {},
     "execution_count": 2
    }
   ],
   "source": [
    "LOG_FILE_PATH = \"results/CartPole-v1/train-a2c/20210405-225734/all_results.txt\"\n",
    "\n",
    "RESULTS = []\n",
    "SEPARATOR = \"==\"\n",
    "\n",
    "with smart_open(LOG_FILE_PATH, \"r\", encoding=\"utf-8\", errors=\"ignore\") as f:\n",
    "    contents = f.readlines()\n",
    "\n",
    "    # Strip unnecessary/empty new lines\n",
    "    contents = [line for line in contents if line != \"\\n\"]\n",
    "    contents = [line for line in contents if not line.startswith(SEPARATOR)]\n",
    "\n",
    "    enumerated_contents = list(enumerate(contents, 1))\n",
    "\n",
    "    for _ in range(0, len(enumerated_contents)-1, 4):\n",
    "        result = {}\n",
    "        result[\"datetime\"] = enumerated_contents[_][1].split(\":\")[1]\n",
    "        result[\"hyperparameters\"] = enumerated_contents[_ + 1][1].split(\":\", 1)[1]\n",
    "        result[\"results\"] = enumerated_contents[_ + 2][1].split(\":\", 1)[1]\n",
    "        result[\"wall_time\"] = enumerated_contents[_ + 3][1].split(\":\")[1]\n",
    "\n",
    "        RESULTS.append(result)        \n",
    "\n",
    "len(RESULTS)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "            datetime                                    hyperparameters                                            results             wall_time\n",
       "0  20210405-225734\\n  num_env:6, num_episodes:50000, learning_rate:0...  min_reward:21.3, max_reward:139.2, reward_vari...  120.04074515500179\\n\n",
       "1  20210405-225734\\n  num_env:6, num_episodes:50000, learning_rate:0...  min_reward:15.1, max_reward:184.4, reward_vari...  126.65153170299891\\n\n",
       "2  20210405-225734\\n  num_env:6, num_episodes:50000, learning_rate:0...  min_reward:15.4, max_reward:202.4, reward_vari...  129.78630696300024\\n\n",
       "3  20210405-225734\\n  num_env:6, num_episodes:50000, learning_rate:0...  min_reward:18.5, max_reward:205.1, reward_vari...   134.3010969499992\\n\n",
       "4  20210405-225734\\n  num_env:6, num_episodes:50000, learning_rate:0...  min_reward:17.0, max_reward:178.5, reward_vari...  130.13781459500024\\n"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>datetime</th>\n      <th>hyperparameters</th>\n      <th>results</th>\n      <th>wall_time</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>20210405-225734\\n</td>\n      <td>num_env:6, num_episodes:50000, learning_rate:0...</td>\n      <td>min_reward:21.3, max_reward:139.2, reward_vari...</td>\n      <td>120.04074515500179\\n</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>20210405-225734\\n</td>\n      <td>num_env:6, num_episodes:50000, learning_rate:0...</td>\n      <td>min_reward:15.1, max_reward:184.4, reward_vari...</td>\n      <td>126.65153170299891\\n</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>20210405-225734\\n</td>\n      <td>num_env:6, num_episodes:50000, learning_rate:0...</td>\n      <td>min_reward:15.4, max_reward:202.4, reward_vari...</td>\n      <td>129.78630696300024\\n</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>20210405-225734\\n</td>\n      <td>num_env:6, num_episodes:50000, learning_rate:0...</td>\n      <td>min_reward:18.5, max_reward:205.1, reward_vari...</td>\n      <td>134.3010969499992\\n</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>20210405-225734\\n</td>\n      <td>num_env:6, num_episodes:50000, learning_rate:0...</td>\n      <td>min_reward:17.0, max_reward:178.5, reward_vari...</td>\n      <td>130.13781459500024\\n</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 100
    }
   ],
   "source": [
    "results_df = pd.DataFrame.from_dict(RESULTS)\n",
    "results_df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "          datetime                                    hyperparameters                                            results           wall_time\n",
       "0  20210405-225734  num_env:6, num_episodes:50000, learning_rate:0...  min_reward:21.3, max_reward:139.2, reward_vari...  120.04074515500179\n",
       "1  20210405-225734  num_env:6, num_episodes:50000, learning_rate:0...  min_reward:15.1, max_reward:184.4, reward_vari...  126.65153170299891\n",
       "2  20210405-225734  num_env:6, num_episodes:50000, learning_rate:0...  min_reward:15.4, max_reward:202.4, reward_vari...  129.78630696300024"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>datetime</th>\n      <th>hyperparameters</th>\n      <th>results</th>\n      <th>wall_time</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>20210405-225734</td>\n      <td>num_env:6, num_episodes:50000, learning_rate:0...</td>\n      <td>min_reward:21.3, max_reward:139.2, reward_vari...</td>\n      <td>120.04074515500179</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>20210405-225734</td>\n      <td>num_env:6, num_episodes:50000, learning_rate:0...</td>\n      <td>min_reward:15.1, max_reward:184.4, reward_vari...</td>\n      <td>126.65153170299891</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>20210405-225734</td>\n      <td>num_env:6, num_episodes:50000, learning_rate:0...</td>\n      <td>min_reward:15.4, max_reward:202.4, reward_vari...</td>\n      <td>129.78630696300024</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 101
    }
   ],
   "source": [
    "# Strip new lines\n",
    "results_df[\"datetime\"] = results_df[\"datetime\"].str.strip()\n",
    "results_df[\"hyperparameters\"] = results_df[\"hyperparameters\"].str.strip()\n",
    "results_df[\"results\"] = results_df[\"results\"].str.strip()\n",
    "results_df[\"results\"] = results_df[\"results\"].str.replace(\"::\", \":-\") # This is to fix a bad replace bug during logging - negative numbers must be preserved\n",
    "results_df[\"wall_time\"] = results_df[\"wall_time\"].str.strip()\n",
    "results_df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "          datetime                                    hyperparameters                                            results           wall_time\n",
       "0  20210405-225734  [num_env:6,  num_episodes:50000,  learning_rat...  [min_reward:21.3,  max_reward:139.2,  reward_v...  120.04074515500179\n",
       "1  20210405-225734  [num_env:6,  num_episodes:50000,  learning_rat...  [min_reward:15.1,  max_reward:184.4,  reward_v...  126.65153170299891\n",
       "2  20210405-225734  [num_env:6,  num_episodes:50000,  learning_rat...  [min_reward:15.4,  max_reward:202.4,  reward_v...  129.78630696300024"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>datetime</th>\n      <th>hyperparameters</th>\n      <th>results</th>\n      <th>wall_time</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>20210405-225734</td>\n      <td>[num_env:6,  num_episodes:50000,  learning_rat...</td>\n      <td>[min_reward:21.3,  max_reward:139.2,  reward_v...</td>\n      <td>120.04074515500179</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>20210405-225734</td>\n      <td>[num_env:6,  num_episodes:50000,  learning_rat...</td>\n      <td>[min_reward:15.1,  max_reward:184.4,  reward_v...</td>\n      <td>126.65153170299891</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>20210405-225734</td>\n      <td>[num_env:6,  num_episodes:50000,  learning_rat...</td>\n      <td>[min_reward:15.4,  max_reward:202.4,  reward_v...</td>\n      <td>129.78630696300024</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 102
    }
   ],
   "source": [
    "# Split items in hyperparameters and results columns into lists\n",
    "results_df[\"hyperparameters\"] = results_df[\"hyperparameters\"].str.split(\",\", 3)\n",
    "results_df[\"results\"] = results_df[\"results\"].str.split(\",\")\n",
    "results_df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make reliable index column\n",
    "results_df.reset_index(level=0, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "flattened_hyperparameters_df = pd.DataFrame(results_df[\"hyperparameters\"].to_list(), columns=[\"num_env\", \"num_episodes\", \"learning_rate\", \"hidden_layers\"], index=results_df[\"index\"])\n",
    "flattened_results_df = pd.DataFrame(results_df[\"results\"].to_list(), columns=[\"min_reward\", \"max_reward\", \"reward_variance\", \"mean_reward\", \"mean_actor_loss\", \"mean_critic_loss\", \"mean_entropy_loss\", \"mean_overall_loss\"], index=results_df[\"index\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "flattened_hyperparameters_df.reset_index(level=0, inplace=True)\n",
    "flattened_results_df.reset_index(level=0, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "   index_r         datetime                                    hyperparameters                                            results  \\\n",
       "0        0  20210405-225734  [num_env:6,  num_episodes:50000,  learning_rat...  [min_reward:21.3,  max_reward:139.2,  reward_v...   \n",
       "1        1  20210405-225734  [num_env:6,  num_episodes:50000,  learning_rat...  [min_reward:15.1,  max_reward:184.4,  reward_v...   \n",
       "2        2  20210405-225734  [num_env:6,  num_episodes:50000,  learning_rat...  [min_reward:15.4,  max_reward:202.4,  reward_v...   \n",
       "\n",
       "            wall_time  index       min_reward         max_reward                      reward_variance                     mean_reward  \\\n",
       "0  120.04074515500179      0  min_reward:21.3   max_reward:139.2   reward_variance:26.496033589954553   mean_reward:77.29800000000002   \n",
       "1  126.65153170299891      1  min_reward:15.1   max_reward:184.4    reward_variance:37.07680115651834   mean_reward:90.30400000000002   \n",
       "2  129.78630696300024      2  min_reward:15.4   max_reward:202.4   reward_variance:35.883354135309034   mean_reward:99.63600000000001   \n",
       "\n",
       "                         mean_actor_loss                      mean_critic_loss                       mean_entropy_loss  \\\n",
       "0   mean_actor_loss:-0.11034903979808929   mean_critic_loss:2.3578196430739373   mean_entropy_loss:-3.2299348449707033   \n",
       "1   mean_actor_loss:-0.09831048473428179   mean_critic_loss:1.9740386339361984    mean_entropy_loss:-3.166618962287903   \n",
       "2   mean_actor_loss:-0.10236293252928909    mean_critic_loss:1.819860874202171   mean_entropy_loss:-3.1372220420837404   \n",
       "\n",
       "                       mean_overall_loss  \n",
       "0   mean_overall_loss:2.2442579316822346  \n",
       "1   mean_overall_loss:1.8725499991586898  \n",
       "2   mean_overall_loss:1.7143505113069666  "
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>index_r</th>\n      <th>datetime</th>\n      <th>hyperparameters</th>\n      <th>results</th>\n      <th>wall_time</th>\n      <th>index</th>\n      <th>min_reward</th>\n      <th>max_reward</th>\n      <th>reward_variance</th>\n      <th>mean_reward</th>\n      <th>mean_actor_loss</th>\n      <th>mean_critic_loss</th>\n      <th>mean_entropy_loss</th>\n      <th>mean_overall_loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0</td>\n      <td>20210405-225734</td>\n      <td>[num_env:6,  num_episodes:50000,  learning_rat...</td>\n      <td>[min_reward:21.3,  max_reward:139.2,  reward_v...</td>\n      <td>120.04074515500179</td>\n      <td>0</td>\n      <td>min_reward:21.3</td>\n      <td>max_reward:139.2</td>\n      <td>reward_variance:26.496033589954553</td>\n      <td>mean_reward:77.29800000000002</td>\n      <td>mean_actor_loss:-0.11034903979808929</td>\n      <td>mean_critic_loss:2.3578196430739373</td>\n      <td>mean_entropy_loss:-3.2299348449707033</td>\n      <td>mean_overall_loss:2.2442579316822346</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1</td>\n      <td>20210405-225734</td>\n      <td>[num_env:6,  num_episodes:50000,  learning_rat...</td>\n      <td>[min_reward:15.1,  max_reward:184.4,  reward_v...</td>\n      <td>126.65153170299891</td>\n      <td>1</td>\n      <td>min_reward:15.1</td>\n      <td>max_reward:184.4</td>\n      <td>reward_variance:37.07680115651834</td>\n      <td>mean_reward:90.30400000000002</td>\n      <td>mean_actor_loss:-0.09831048473428179</td>\n      <td>mean_critic_loss:1.9740386339361984</td>\n      <td>mean_entropy_loss:-3.166618962287903</td>\n      <td>mean_overall_loss:1.8725499991586898</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2</td>\n      <td>20210405-225734</td>\n      <td>[num_env:6,  num_episodes:50000,  learning_rat...</td>\n      <td>[min_reward:15.4,  max_reward:202.4,  reward_v...</td>\n      <td>129.78630696300024</td>\n      <td>2</td>\n      <td>min_reward:15.4</td>\n      <td>max_reward:202.4</td>\n      <td>reward_variance:35.883354135309034</td>\n      <td>mean_reward:99.63600000000001</td>\n      <td>mean_actor_loss:-0.10236293252928909</td>\n      <td>mean_critic_loss:1.819860874202171</td>\n      <td>mean_entropy_loss:-3.1372220420837404</td>\n      <td>mean_overall_loss:1.7143505113069666</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 128
    }
   ],
   "source": [
    "preprocessed_results_df = results_df.join(flattened_hyperparameters_df, on=\"index\", lsuffix=\"_p\")\n",
    "preprocessed_results_df = results_df.join(flattened_results_df, on=\"index\", lsuffix=\"_r\")\n",
    "preprocessed_results_df.head(3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "          datetime           wall_time  index       min_reward         max_reward                      reward_variance  \\\n",
       "0  20210405-225734  120.04074515500179      0  min_reward:21.3   max_reward:139.2   reward_variance:26.496033589954553   \n",
       "1  20210405-225734  126.65153170299891      1  min_reward:15.1   max_reward:184.4    reward_variance:37.07680115651834   \n",
       "\n",
       "                      mean_reward                        mean_actor_loss                      mean_critic_loss  \\\n",
       "0   mean_reward:77.29800000000002   mean_actor_loss:-0.11034903979808929   mean_critic_loss:2.3578196430739373   \n",
       "1   mean_reward:90.30400000000002   mean_actor_loss:-0.09831048473428179   mean_critic_loss:1.9740386339361984   \n",
       "\n",
       "                        mean_entropy_loss                      mean_overall_loss  \n",
       "0   mean_entropy_loss:-3.2299348449707033   mean_overall_loss:2.2442579316822346  \n",
       "1    mean_entropy_loss:-3.166618962287903   mean_overall_loss:1.8725499991586898  "
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>datetime</th>\n      <th>wall_time</th>\n      <th>index</th>\n      <th>min_reward</th>\n      <th>max_reward</th>\n      <th>reward_variance</th>\n      <th>mean_reward</th>\n      <th>mean_actor_loss</th>\n      <th>mean_critic_loss</th>\n      <th>mean_entropy_loss</th>\n      <th>mean_overall_loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>20210405-225734</td>\n      <td>120.04074515500179</td>\n      <td>0</td>\n      <td>min_reward:21.3</td>\n      <td>max_reward:139.2</td>\n      <td>reward_variance:26.496033589954553</td>\n      <td>mean_reward:77.29800000000002</td>\n      <td>mean_actor_loss:-0.11034903979808929</td>\n      <td>mean_critic_loss:2.3578196430739373</td>\n      <td>mean_entropy_loss:-3.2299348449707033</td>\n      <td>mean_overall_loss:2.2442579316822346</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>20210405-225734</td>\n      <td>126.65153170299891</td>\n      <td>1</td>\n      <td>min_reward:15.1</td>\n      <td>max_reward:184.4</td>\n      <td>reward_variance:37.07680115651834</td>\n      <td>mean_reward:90.30400000000002</td>\n      <td>mean_actor_loss:-0.09831048473428179</td>\n      <td>mean_critic_loss:1.9740386339361984</td>\n      <td>mean_entropy_loss:-3.166618962287903</td>\n      <td>mean_overall_loss:1.8725499991586898</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 129
    }
   ],
   "source": [
    "preprocessed_results_df = preprocessed_results_df.drop(columns=[\"index_r\", \"hyperparameters\", \"results\"])\n",
    "preprocessed_results_df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "   index         datetime           wall_time       min_reward         max_reward                     mean_reward  \\\n",
       "0      0  20210405-225734  120.04074515500179  min_reward:21.3   max_reward:139.2   mean_reward:77.29800000000002   \n",
       "\n",
       "                       reward_variance                        mean_actor_loss                      mean_critic_loss  \\\n",
       "0   reward_variance:26.496033589954553   mean_actor_loss:-0.11034903979808929   mean_critic_loss:2.3578196430739373   \n",
       "\n",
       "                        mean_entropy_loss                      mean_overall_loss  \n",
       "0   mean_entropy_loss:-3.2299348449707033   mean_overall_loss:2.2442579316822346  "
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>index</th>\n      <th>datetime</th>\n      <th>wall_time</th>\n      <th>min_reward</th>\n      <th>max_reward</th>\n      <th>mean_reward</th>\n      <th>reward_variance</th>\n      <th>mean_actor_loss</th>\n      <th>mean_critic_loss</th>\n      <th>mean_entropy_loss</th>\n      <th>mean_overall_loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0</td>\n      <td>20210405-225734</td>\n      <td>120.04074515500179</td>\n      <td>min_reward:21.3</td>\n      <td>max_reward:139.2</td>\n      <td>mean_reward:77.29800000000002</td>\n      <td>reward_variance:26.496033589954553</td>\n      <td>mean_actor_loss:-0.11034903979808929</td>\n      <td>mean_critic_loss:2.3578196430739373</td>\n      <td>mean_entropy_loss:-3.2299348449707033</td>\n      <td>mean_overall_loss:2.2442579316822346</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 130
    }
   ],
   "source": [
    "_columns = [\"index\", \"datetime\", \"wall_time\", \"min_reward\", \"max_reward\", \"mean_reward\", \"reward_variance\", \"mean_actor_loss\", \"mean_critic_loss\", \"mean_entropy_loss\", \"mean_overall_loss\"]\n",
    "\n",
    "preprocessed_results_df = preprocessed_results_df[_columns]\n",
    "preprocessed_results_df.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(54, 11)"
      ]
     },
     "metadata": {},
     "execution_count": 131
    }
   ],
   "source": [
    "preprocessed_results_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "   index         datetime           wall_time min_reward max_reward        mean_reward     reward_variance       mean_actor_loss  \\\n",
       "0      0  20210405-225734  120.04074515500179       21.3      139.2  77.29800000000002  26.496033589954553  -0.11034903979808929   \n",
       "1      1  20210405-225734  126.65153170299891       15.1      184.4  90.30400000000002   37.07680115651834  -0.09831048473428179   \n",
       "2      2  20210405-225734  129.78630696300024       15.4      202.4  99.63600000000001  35.883354135309034  -0.10236293252928909   \n",
       "\n",
       "     mean_critic_loss    mean_entropy_loss   mean_overall_loss  \n",
       "0  2.3578196430739373  -3.2299348449707033  2.2442579316822346  \n",
       "1  1.9740386339361984   -3.166618962287903  1.8725499991586898  \n",
       "2   1.819860874202171  -3.1372220420837404  1.7143505113069666  "
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>index</th>\n      <th>datetime</th>\n      <th>wall_time</th>\n      <th>min_reward</th>\n      <th>max_reward</th>\n      <th>mean_reward</th>\n      <th>reward_variance</th>\n      <th>mean_actor_loss</th>\n      <th>mean_critic_loss</th>\n      <th>mean_entropy_loss</th>\n      <th>mean_overall_loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0</td>\n      <td>20210405-225734</td>\n      <td>120.04074515500179</td>\n      <td>21.3</td>\n      <td>139.2</td>\n      <td>77.29800000000002</td>\n      <td>26.496033589954553</td>\n      <td>-0.11034903979808929</td>\n      <td>2.3578196430739373</td>\n      <td>-3.2299348449707033</td>\n      <td>2.2442579316822346</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1</td>\n      <td>20210405-225734</td>\n      <td>126.65153170299891</td>\n      <td>15.1</td>\n      <td>184.4</td>\n      <td>90.30400000000002</td>\n      <td>37.07680115651834</td>\n      <td>-0.09831048473428179</td>\n      <td>1.9740386339361984</td>\n      <td>-3.166618962287903</td>\n      <td>1.8725499991586898</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2</td>\n      <td>20210405-225734</td>\n      <td>129.78630696300024</td>\n      <td>15.4</td>\n      <td>202.4</td>\n      <td>99.63600000000001</td>\n      <td>35.883354135309034</td>\n      <td>-0.10236293252928909</td>\n      <td>1.819860874202171</td>\n      <td>-3.1372220420837404</td>\n      <td>1.7143505113069666</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 132
    }
   ],
   "source": [
    "# Remove text from each column\n",
    "preprocessed_results_df[\"min_reward\"] = preprocessed_results_df[\"min_reward\"].str.split(\":\").str[1]\n",
    "preprocessed_results_df[\"max_reward\"] = preprocessed_results_df[\"max_reward\"].str.split(\":\").str[1]\n",
    "preprocessed_results_df[\"mean_reward\"] = preprocessed_results_df[\"mean_reward\"].str.split(\":\").str[1]\n",
    "preprocessed_results_df[\"reward_variance\"] = preprocessed_results_df[\"reward_variance\"].str.split(\":\").str[1]\n",
    "preprocessed_results_df[\"mean_actor_loss\"] = preprocessed_results_df[\"mean_actor_loss\"].str.split(\":\").str[1]\n",
    "preprocessed_results_df[\"mean_critic_loss\"] = preprocessed_results_df[\"mean_critic_loss\"].str.split(\":\").str[1]\n",
    "preprocessed_results_df[\"mean_entropy_loss\"] = preprocessed_results_df[\"mean_entropy_loss\"].str.split(\":\").str[1]\n",
    "preprocessed_results_df[\"mean_overall_loss\"] = preprocessed_results_df[\"mean_overall_loss\"].str.split(\":\").str[1]\n",
    "preprocessed_results_df.head(3)"
   ]
  },
  {
   "source": [
    "# Save results to CSV\n",
    "preprocessed_results_df.to_csv(\"training_results_20210405-225734.csv\", index=False)"
   ],
   "cell_type": "code",
   "metadata": {},
   "execution_count": 133,
   "outputs": []
  },
  {
   "source": [
    "## Analyse training results"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}