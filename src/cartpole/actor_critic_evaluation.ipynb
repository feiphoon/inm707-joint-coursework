{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3",
   "language": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# Actor-Critic evaluation\n",
    "\n",
    "Plan:\n",
    "- ~~Spike Actor-Critic~~\n",
    "- ~~Quick test on a Gym env~~\n",
    "- ~~Update requirements.txt~~\n",
    "- ~~Move Actor-Critic out~~\n",
    "- ~~Make training records for analysis~~\n",
    "- Structure next code"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Imports & setup"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### Essential tools"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generic setup\n",
    "import math\n",
    "from typing import Tuple, List, Callable\n",
    "from collections import namedtuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analysis\n",
    "from IPython.display import clear_output\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "source": [
    "### Examine Gym environments"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# %%capture\n",
    "# from gym import envs\n",
    "# print(envs.registry.all())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(500,\n",
       " 475.0,\n",
       " Discrete(2),\n",
       " Box(-3.4028234663852886e+38, 3.4028234663852886e+38, (4,), float32))"
      ]
     },
     "metadata": {},
     "execution_count": 11
    }
   ],
   "source": [
    "# Smoke test\n",
    "env = gym.make(\"CartPole-v1\")\n",
    "# env = gym.make(\"Pong-v0\")\n",
    "# # Check environment details\n",
    "# CartPole-v0 is 200, 195.0\n",
    "# CartPole-v1 is 500, 475.0\n",
    "env.spec.max_episode_steps, env.spec.reward_threshold, env.action_space, env.observation_space\n",
    "\n",
    "# Rememeber to make reproducible gym environments\n",
    "# env.seed(0)"
   ]
  },
  {
   "source": [
    "### Import PyTorch"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x135595590>"
      ]
     },
     "metadata": {},
     "execution_count": 6
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# Check for CUDA\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Reproducible results\n",
    "torch.manual_seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Requirement already satisfied: stable_baselines3==1.0 in /Users/fei/.pyenv/versions/3.8.5/lib/python3.8/site-packages (1.0)\n",
      "Requirement already satisfied: torch>=1.4.0 in /Users/fei/.pyenv/versions/3.8.5/lib/python3.8/site-packages (from stable_baselines3==1.0) (1.8.0)\n",
      "Requirement already satisfied: gym>=0.17 in /Users/fei/.pyenv/versions/3.8.5/lib/python3.8/site-packages (from stable_baselines3==1.0) (0.18.0)\n",
      "Requirement already satisfied: pandas in /Users/fei/.pyenv/versions/3.8.5/lib/python3.8/site-packages (from stable_baselines3==1.0) (1.2.1)\n",
      "Requirement already satisfied: matplotlib in /Users/fei/.pyenv/versions/3.8.5/lib/python3.8/site-packages (from stable_baselines3==1.0) (3.3.4)\n",
      "Requirement already satisfied: numpy in /Users/fei/.pyenv/versions/3.8.5/lib/python3.8/site-packages (from stable_baselines3==1.0) (1.20.0)\n",
      "Requirement already satisfied: cloudpickle in /Users/fei/.pyenv/versions/3.8.5/lib/python3.8/site-packages (from stable_baselines3==1.0) (1.6.0)\n",
      "Requirement already satisfied: pyglet<=1.5.0,>=1.4.0 in /Users/fei/.pyenv/versions/3.8.5/lib/python3.8/site-packages (from gym>=0.17->stable_baselines3==1.0) (1.5.0)\n",
      "Requirement already satisfied: Pillow<=7.2.0 in /Users/fei/.pyenv/versions/3.8.5/lib/python3.8/site-packages (from gym>=0.17->stable_baselines3==1.0) (7.2.0)\n",
      "Requirement already satisfied: scipy in /Users/fei/.pyenv/versions/3.8.5/lib/python3.8/site-packages (from gym>=0.17->stable_baselines3==1.0) (1.5.4)\n",
      "Requirement already satisfied: future in /Users/fei/.pyenv/versions/3.8.5/lib/python3.8/site-packages (from pyglet<=1.5.0,>=1.4.0->gym>=0.17->stable_baselines3==1.0) (0.18.2)\n",
      "Requirement already satisfied: typing-extensions in /Users/fei/.pyenv/versions/3.8.5/lib/python3.8/site-packages (from torch>=1.4.0->stable_baselines3==1.0) (3.7.4.3)\n",
      "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.3 in /Users/fei/.pyenv/versions/3.8.5/lib/python3.8/site-packages (from matplotlib->stable_baselines3==1.0) (2.4.7)\n",
      "Requirement already satisfied: cycler>=0.10 in /Users/fei/.pyenv/versions/3.8.5/lib/python3.8/site-packages (from matplotlib->stable_baselines3==1.0) (0.10.0)\n",
      "Requirement already satisfied: python-dateutil>=2.1 in /Users/fei/.pyenv/versions/3.8.5/lib/python3.8/site-packages (from matplotlib->stable_baselines3==1.0) (2.8.1)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /Users/fei/.pyenv/versions/3.8.5/lib/python3.8/site-packages (from matplotlib->stable_baselines3==1.0) (1.3.1)\n",
      "Requirement already satisfied: six in /Users/fei/.pyenv/versions/3.8.5/lib/python3.8/site-packages (from cycler>=0.10->matplotlib->stable_baselines3==1.0) (1.15.0)\n",
      "Requirement already satisfied: pytz>=2017.3 in /Users/fei/.pyenv/versions/3.8.5/lib/python3.8/site-packages (from pandas->stable_baselines3==1.0) (2020.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install stable_baselines3==1.0\n",
    "\n",
    "import stable_baselines3\n",
    "from stable_baselines3.common.env_util import SubprocVecEnv\n",
    "from stable_baselines3.common.utils import set_random_seed"
   ]
  },
  {
   "source": [
    "### Import local modules"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "from actor_critic import ActorCritic"
   ]
  },
  {
   "source": [
    "## Set up evaluation\n",
    "\n",
    "https://towardsdatascience.com/understanding-actor-critic-methods-931b97b6df3f\n",
    "\n",
    "Doing A2C, a single worker variant of A3C.\n",
    "- backprop\n",
    "- keep the hidden layer simple\n",
    "- RMSprop as loss function\n",
    "- reward step size/evaluation step size 5\n",
    "\n",
    "Hyperparameters:\n",
    "(values from Deep Reinforcement Learning Hands-On, Maxim Lapan)\n",
    "- num of envs used [20, 40, 60, 80, 100] <-- do 5 separate big experiments\n",
    "- batch size [16, 32, 64]\n",
    "- learning rate [0.001, 0.002, 0.003]\n",
    "- entropy beta? [0.02, 0.03]\n",
    "- hidden layers? (nah)\n",
    "\n",
    "Analysis:\n",
    "- Average reward\n",
    "- No. of timesteps per episode (before terminating state)\n",
    "- entropy loss?\n",
    "- policy loss\n",
    "- value loss\n",
    "- (implied) Collect reward, timesteps elapsed\n",
    "\n",
    "Gym CartPole V0 & V1 description:\n",
    "\n",
    "\n",
    ">A pole is attached by an un-actuated joint to a cart, which moves along a frictionless track. The system is controlled by applying a force of +1 or -1 to the cart. The pendulum starts upright, and the goal is to prevent it from falling over. A reward of +1 is provided for every timestep that the pole remains upright. The episode ends when the pole is more than 15 degrees from vertical, or the cart moves more than 2.4 units from the center.\n",
    "\n",
    "\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_rewards_episodes(experiment_rewards: list):\n",
    "    clear_output(True)\n",
    "    plt.figure(figsize=(20,5))\n",
    "    plt.subplot(131)\n",
    "    plt.title(\"episodes vs reward\")\n",
    "    plt.plot(reward)\n",
    "    plt.show()\n",
    "    \n",
    "def plot_timesteps_episodes(timesteps_elapsed: list):\n",
    "    clear_output(True)\n",
    "    plt.figure(figsize=(20,5))\n",
    "    plt.subplot(141)\n",
    "    plt.title(\"episodes vs timesteps\")\n",
    "    plt.plot(timesteps_elapsed)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# One episode/test run/logging rewards\n",
    "def sample_one_episode(env: gym.Env, model: ActorCritic):\n",
    "    state = env.reset()\n",
    "    done = False\n",
    "\n",
    "    total_reward = 0\n",
    "    timestep_counter = 0\n",
    "\n",
    "    while not done:\n",
    "        state = torch.unsqueeze(torch.FloatTensor(state), 0).to(device)\n",
    "        probability_dist, values = model(state)\n",
    "        action_to_take = probability_dist.sample()\n",
    "        next_state, reward, done, _ = env.step(action_to_take.cpu().detach().numpy()[0])\n",
    "        state = next_state\n",
    "\n",
    "        total_reward += reward\n",
    "        timestep_counter += 1\n",
    "\n",
    "    return total_reward, timestep_counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_returns(next_value: torch.Tensor, rewards: List[torch.FloatTensor], masks: List[int], gamma: float) -> List[torch.Tensor]:\n",
    "    calculated_returns = []\n",
    "    # Calculate the accumulated returns \n",
    "    # from the number of \"reward steps to update\".\n",
    "    # Reset R to the next_value first.\n",
    "    R = next_value\n",
    "\n",
    "    # Calculate discounted return & go backwards\n",
    "    for _ in range(len(rewards))[::-1]:\n",
    "        R = rewards[_] + gamma * R * masks[step]\n",
    "        # Push return value R\n",
    "        calculated_returns.insert(0, R)\n",
    "    return calculated_returns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "NameError",
     "evalue": "name 'set_random_seed' is not defined",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-22-bb4dace686f4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;31m# Create the vectorized environment\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m \u001b[0menvs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSubprocVecEnv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmake_env\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"CartPole-v1\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mNUM_CPU\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mActorCritic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_inputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mNUM_INPUTS\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_outputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mNUM_OUTPUTS\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden_layer_config\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-22-bb4dace686f4>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;31m# Create the vectorized environment\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m \u001b[0menvs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSubprocVecEnv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmake_env\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"CartPole-v1\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mNUM_CPU\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mActorCritic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_inputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mNUM_INPUTS\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_outputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mNUM_OUTPUTS\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden_layer_config\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-22-bb4dace686f4>\u001b[0m in \u001b[0;36mmake_env\u001b[0;34m(env_id, rank, seed)\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mseed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseed\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mrank\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m     \u001b[0mset_random_seed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseed\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0m_init\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'set_random_seed' is not defined"
     ]
    }
   ],
   "source": [
    "# From: https://colab.research.google.com/github/Stable-Baselines-Team/rl-colab-notebooks/blob/sb3/multiprocessing_rl.ipynb\n",
    "\n",
    "def make_env(env_id: str, rank: int, seed: int = 0) -> Callable:\n",
    "    \"\"\"\n",
    "    Utility function for multiprocessed env.\n",
    "    \n",
    "    :param env_id: (str) the environment ID\n",
    "    :param num_env: (int) the number of environment you wish to have in subprocesses\n",
    "    :param seed: (int) the inital seed for RNG\n",
    "    :param rank: (int) index of the subprocess\n",
    "    :return: (Callable)\n",
    "    \"\"\"\n",
    "    def _init() -> gym.Env:\n",
    "        env = gym.make(env_id)\n",
    "        env.seed(seed + rank)\n",
    "        return env\n",
    "    set_random_seed(seed)\n",
    "    return _init\n",
    "\n",
    "\n",
    "# Eduard = MSI RX 2080, 6 cores Intel Core i5 9600KF\n",
    "# Fei = 6 cores AMD Ryzen 5 3600, I'm not going to install ROCm\n",
    "NUM_CPU = 6  # Number of processes to use\n",
    "\n",
    "# Parameters based on environment:\n",
    "NUM_OBSERVATIONS: int = env.observation_space.shape[0] # input\n",
    "NUM_ACTIONS: int = env.action_space.n # output\n",
    "\n",
    "\n",
    "LEARNING_RATE_LIST = [0.001, 0.002, 0.003]\n",
    "\n",
    "# Create the vectorized environment\n",
    "envs = SubprocVecEnv([make_env(\"CartPole-v1\", i) for i in range(NUM_CPU)])\n",
    "\n",
    "model = ActorCritic(num_inputs=NUM_INPUTS, num_outputs=NUM_OUTPUTS, hidden_layer_config=(10, 10)).to(device)\n",
    "optimiser = optim.RMSprop(model.parameters(), lr=0.001) # TODO\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "output_type": "error",
     "ename": "AttributeError",
     "evalue": "'int' object has no attribute '_comparable'",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-18-f7bc5264b0bb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0mexperiment_timesteps\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m \u001b[0;32mwhile\u001b[0m \u001b[0mepisode_idx\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mNUM_EPISODES\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0;31m# Structures to hold our records for updating\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.8.5/lib/python3.8/site-packages/tqdm/utils.py\u001b[0m in \u001b[0;36m__gt__\u001b[0;34m(self, other)\u001b[0m\n\u001b[1;32m     78\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__gt__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mother\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 80\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0mother\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     81\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__ge__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mother\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.8.5/lib/python3.8/site-packages/tqdm/utils.py\u001b[0m in \u001b[0;36m__le__\u001b[0;34m(self, other)\u001b[0m\n\u001b[1;32m     69\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__le__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mother\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 71\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mother\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mother\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     72\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__eq__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mother\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.8.5/lib/python3.8/site-packages/tqdm/utils.py\u001b[0m in \u001b[0;36m__lt__\u001b[0;34m(self, other)\u001b[0m\n\u001b[1;32m     66\u001b[0m     \u001b[0;34m\"\"\"Assumes child has self._comparable attr/@property\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__lt__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mother\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 68\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_comparable\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mother\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_comparable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     69\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__le__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mother\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'int' object has no attribute '_comparable'"
     ]
    }
   ],
   "source": [
    "# Three terminating states for CartPole-v1:\n",
    "# - env.spec.max_episode_steps 500\n",
    "# - env.spec.reward_threshold 475.0\n",
    "# - falls over\n",
    "\n",
    "ENTROPY_REGULARISATION = 0.01 # Entropy regularisation weight/beta/coefficient\n",
    "VALUE_LOSS_COEFFICIENT = 0.1 # Have seen this as 0.5 and 0.1, we will choose the lower\n",
    "NUM_EPISODES = 10000\n",
    "NUM_REWARD_STEPS = 5\n",
    "episode_idx = 0\n",
    "experiment_rewards = []\n",
    "experiment_timesteps = []\n",
    "\n",
    "while episode_idx < tqdm(NUM_EPISODES):\n",
    "\n",
    "    # Structures to hold our records for updating\n",
    "    log_probs = []\n",
    "    values = []\n",
    "    rewards = []\n",
    "\n",
    "    entropy = 0 # reset Entropy\n",
    "    # Our thresholding tensors which \"turn off\" returns\n",
    "    # for the next_value (changes them to zero),\n",
    "    # if the episode terminates before the 500 max \n",
    "    # timesteps on Cartpole-v1 are reached.\n",
    "    masks = []\n",
    "\n",
    "    for _ in range(NUM_REWARD_STEPS):\n",
    "        state = torch.unsqueeze(torch.FloatTensor(state), 0).to(device)\n",
    "        probability_distribution, state_values = model(state)\n",
    "        action_to_take = probability_distribution.sample()\n",
    "\n",
    "        next_state, reward, done, _ = env.step(action_to_take.cpu().detach().numpy())\n",
    "        state = next_state\n",
    "\n",
    "        # Update all the things:\n",
    "        values.append(state_values)\n",
    "\n",
    "        log_prob = probability_distribution.log_prod(action_to_take)\n",
    "        log_probs.append(log_prob)\n",
    "\n",
    "        rewards.append(torch.unsqueeze(torch.FloatTensor(reward),1).to(device))\n",
    "\n",
    "        entropy += probability_distribution.entropy().mean()\n",
    "\n",
    "        # 1 - False = 1; 1 - True = 0\n",
    "        # 1 - returns for next value continue to be calculated.\n",
    "        masks.append(torch.unsqueeze(torch.FloatTensor(1 - done),1).to(device))\n",
    "\n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        total_reward += reward\n",
    "        timestep_counter += 1\n",
    "\n",
    "\n",
    "    reward, timesteps_elapsed = sample_one_episode(cartpole)\n",
    "    plot_rewards_episodes(results_list)\n",
    "    # plot_timesteps_episodes(timesteps_elapsed)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}