{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3",
   "language": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# Actor-Critic evaluation\n",
    "\n",
    "Plan:\n",
    "- ~~Spike Actor-Critic~~\n",
    "- ~~Quick test on a Gym env~~\n",
    "- ~~Update requirements.txt~~\n",
    "- ~~Move Actor-Critic out~~\n",
    "- ~~Make training records for analysis~~\n",
    "- Structure next code"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Imports & setup"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### Essential tools"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generic setup\n",
    "from typing import Tuple, List, Callable\n",
    "from collections import namedtuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analysis\n",
    "from IPython.display import clear_output\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "source": [
    "### Examine Gym environments"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# %%capture\n",
    "# from gym import envs\n",
    "# print(envs.registry.all())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Smoke test\n",
    "# env = gym.make(\"CartPole-v1\")\n",
    "# # Check environment details\n",
    "# CartPole-v0 is 200, 195.0\n",
    "# CartPole-v1 is 500, 475.0\n",
    "# env.spec.max_episode_steps, env.spec.reward_threshold, env.action_space, env.observation_space\n",
    "\n",
    "# Rememeber to make reproducible gym environments\n",
    "# env.seed(0)"
   ]
  },
  {
   "source": [
    "### Import PyTorch"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x127f8f330>"
      ]
     },
     "metadata": {},
     "execution_count": 47
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# Check for CUDA\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Reproducible results\n",
    "torch.manual_seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-61-db22bd9e97b1>, line 1)",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-61-db22bd9e97b1>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    import stable-baselines3\u001b[0m\n\u001b[0m                 ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "import stable-baselines3\n",
    "from stable_baselines3.common.env_util import make_vec_env"
   ]
  },
  {
   "source": [
    "### Import local modules"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "from actor_critic import ActorCritic"
   ]
  },
  {
   "source": [
    "## Set up evaluation\n",
    "\n",
    "https://towardsdatascience.com/understanding-actor-critic-methods-931b97b6df3f\n",
    "\n",
    "Doing A2C, a single worker variant of A3C.\n",
    "- backprop\n",
    "- keep the hidden layer simple\n",
    "- RMSprop as loss function\n",
    "- reward step size/evaluation step size 5\n",
    "\n",
    "Hyperparameters:\n",
    "(values from Deep Reinforcement Learning Hands-On, Maxim Lapan)\n",
    "- num of envs used [20, 40, 60, 80, 100] <-- do 5 separate big experiments\n",
    "- batch size [16, 32, 64]\n",
    "- learning rate [0.001, 0.002, 0.003]\n",
    "- entropy beta? [0.02, 0.03]\n",
    "- hidden layers? (nah)\n",
    "\n",
    "Analysis:\n",
    "- Average reward\n",
    "- No. of timesteps per episode (before terminating state)\n",
    "- entropy loss?\n",
    "- policy loss\n",
    "- value loss\n",
    "- (implied) Collect reward, timesteps elapsed\n",
    "\n",
    "Gym CartPole V0 & V1 description:\n",
    "\n",
    "\n",
    ">A pole is attached by an un-actuated joint to a cart, which moves along a frictionless track. The system is controlled by applying a force of +1 or -1 to the cart. The pendulum starts upright, and the goal is to prevent it from falling over. A reward of +1 is provided for every timestep that the pole remains upright. The episode ends when the pole is more than 15 degrees from vertical, or the cart moves more than 2.4 units from the center.\n",
    "\n",
    "\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "Results = namedtuple(\"Results\", \"episode_index, timesteps_elapsed, total_reward_per_episode\")\n",
    "\n",
    "def plot_rewards_episodes(experiment_rewards: list):\n",
    "    clear_output(True)\n",
    "    plt.figure(figsize=(20,5))\n",
    "    plt.subplot(131)\n",
    "    plt.title(\"episodes vs reward\")\n",
    "    plt.plot(reward)\n",
    "    plt.show()\n",
    "    \n",
    "def plot_timesteps_episodes(timesteps_elapsed: list):\n",
    "    clear_output(True)\n",
    "    plt.figure(figsize=(20,5))\n",
    "    plt.subplot(141)\n",
    "    plt.title(\"episodes vs timesteps\")\n",
    "    plt.plot(timesteps_elapsed)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# One episode/test run/logging rewards\n",
    "def sample_one_episode(env: gym.Env, model: ActorCritic):\n",
    "    state = env.reset()\n",
    "    done = False\n",
    "\n",
    "    total_reward = 0\n",
    "    timestep_counter = 0\n",
    "\n",
    "    while not done:\n",
    "        state = torch.unsqueeze(torch.FloatTensor(state), 0).to(device)\n",
    "        probability_dist, values = model(state)\n",
    "        action_to_take = probability_dist.sample()\n",
    "        next_state, reward, done, _ = env.step(action_to_take.cpu().detach().numpy()[0])\n",
    "        state = next_state\n",
    "\n",
    "        total_reward += reward\n",
    "        timestep_counter += 1\n",
    "\n",
    "    return total_reward, timestep_counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_returns(next_value: torch.Tensor, rewards: List[torch.FloatTensor], masks: List[torch.FloatTensor], gamma: float) -> List[torch.Tensor]:\n",
    "    calculated_returns = []\n",
    "    # Calculate the accumulated returns \n",
    "    # from the number of \"reward steps to update\".\n",
    "    # Reset R to the next_value first.\n",
    "    R = next_value\n",
    "\n",
    "    # Calculate discounted return & go backwards\n",
    "    for _ in range(len(rewards))[::-1]:\n",
    "        # TODO: masks?\n",
    "        R = rewards[_] + gamma * R * masks[step]\n",
    "        # Push return value R\n",
    "        calculated_returns.insert(0, R)\n",
    "    return calculated_returns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# From: https://colab.research.google.com/github/Stable-Baselines-Team/rl-colab-notebooks/blob/sb3/multiprocessing_rl.ipynb\n",
    "\n",
    "def make_env(env_id: str, rank: int, seed: int = 0) -> Callable:\n",
    "    \"\"\"\n",
    "    Utility function for multiprocessed env.\n",
    "    \n",
    "    :param env_id: (str) the environment ID\n",
    "    :param num_env: (int) the number of environment you wish to have in subprocesses\n",
    "    :param seed: (int) the inital seed for RNG\n",
    "    :param rank: (int) index of the subprocess\n",
    "    :return: (Callable)\n",
    "    \"\"\"\n",
    "    def _init() -> gym.Env:\n",
    "        env = gym.make(env_id)\n",
    "        env.seed(seed + rank)\n",
    "        return env\n",
    "    set_random_seed(seed)\n",
    "    return _init\n",
    "\n",
    "\n",
    "# Eduard = MSI RX 2080, 6 cores Intel Core i5 9600KF\n",
    "# Fei = 6 cores AMD Ryzen 5 3600, I'm not going to install ROCm\n",
    "NUM_CPU = 6  # Number of processes to use\n",
    "\n",
    "# Parameters based on environment:\n",
    "NUM_OBSERVATIONS: int = env.observation_space.shape[0] # input\n",
    "NUM_ACTIONS: int = env.action_space.n # output\n",
    "\n",
    "# Create the vectorized environment\n",
    "envs = SubprocVecEnv([make_env(\"CartPole-v1\", i) for i in range(NUM_CPU)])\n",
    "\n",
    "model = ActorCritic(num_inputs=NUM_INPUTS, num_outputs=NUM_OUTPUTS, hidden_layer_config=(10, 10)).to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Three terminating states for CartPole-v1:\n",
    "# env.spec.max_episode_steps 500\n",
    "# env.spec.reward_threshold 475.0\n",
    "# falls over\n",
    "\n",
    "NUM_EPISODES = 10000 # Recommended in the Pytorch example for A2C on CartPole-v0\n",
    "episode_idx = 0\n",
    "experiment_rewards = []\n",
    "experiment_timesteps = []\n",
    "\n",
    "while episode_idx < tqdm(NUM_EPISODES):\n",
    "\n",
    "    log_probs = []\n",
    "    values = []\n",
    "    rewards = []\n",
    "    masks = [] # Our thresholding tensors\n",
    "    entropy = 0 # reset Entropy\n",
    "\n",
    "    for _ in range()\n",
    "\n",
    "\n",
    "    reward, timesteps_elapsed = sample_one_episode(cartpole)\n",
    "    plot_rewards_episodes(results_list)\n",
    "    # plot_timesteps_episodes(timesteps_elapsed)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}