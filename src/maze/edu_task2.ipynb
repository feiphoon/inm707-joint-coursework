{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.7"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3",
   "language": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "\"\"\"Q-learning is a model-free reinforcement learning algorithm to learn the value of an action in a particular state. It does not require a model of the environment (hence \"model-free\"), and it can handle problems with stochastic transitions and rewards without requiring adaptations.\n",
    "\n",
    "https://en.wikipedia.org/wiki/Q-learning\"\"\""
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Collecting torchvision\n",
      "  Downloading torchvision-0.9.1-cp38-cp38-macosx_10_9_x86_64.whl (13.2 MB)\n",
      "\u001b[K     |████████████████████████████████| 13.2 MB 8.7 MB/s \n",
      "\u001b[?25hRequirement already satisfied: numpy in /Users/dark_prince/Desktop/DL/Coursework_INM707/inm707-joint-coursework/venv/lib/python3.8/site-packages (from torchvision) (1.20.0)\n",
      "Requirement already satisfied: pillow>=4.1.1 in /Users/dark_prince/Desktop/DL/Coursework_INM707/inm707-joint-coursework/venv/lib/python3.8/site-packages (from torchvision) (8.1.0)\n",
      "Collecting torch==1.8.1\n",
      "  Downloading torch-1.8.1-cp38-none-macosx_10_9_x86_64.whl (119.6 MB)\n",
      "\u001b[K     |████████████████████████████████| 119.6 MB 49.4 MB/s \n",
      "\u001b[?25hRequirement already satisfied: typing-extensions in /Users/dark_prince/Desktop/DL/Coursework_INM707/inm707-joint-coursework/venv/lib/python3.8/site-packages (from torch==1.8.1->torchvision) (3.7.4.3)\n",
      "Installing collected packages: torch, torchvision\n",
      "Successfully installed torch-1.8.1 torchvision-0.9.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "#imports used for task2 \n",
    "import torch\n",
    "is_cuda_available = torch.cuda.is_available()\n",
    "device = torch.device(\"cuda\" if is_cuda_available else \"cpu\")\n",
    "print(device)\n",
    "from typing import Tuple\n",
    "from q_maze import QMaze, Action\n",
    "import numpy as np\n",
    "import pandas as pd \n",
    "from e_greedy_pol import E_greedy_policy\n",
    "import random as random"
   ]
  },
  {
   "source": [
    "### Computing action value functions using E_greedy_policy\n",
    "\n",
    "For this method we will use the E_greedy_policy to help us compute and estimate the **q-value** of each state.\n",
    "\n",
    "The **q-value** is the **mean** expected future reward following an action from a given state. Rather than storing all of our experience and taking the mean over them, we can use each experience to update an exponentially weighted average forget that exprience.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Epsilon-Greedy is a simple method to balance exploration and exploitation by choosing between exploration and exploitation randomly\n",
    "class E_greedy_policy:\n",
    "    def __init__(self, epsilon, decay):\n",
    "\n",
    "        self.epsilon = epsilon #initial value of epsilon\n",
    "        self.epsilon_start = epsilon\n",
    "        self.decay = decay #parameter used to control how much the agent should explore and exploit when using epislon-greedy policy.\n",
    "\n",
    "    # This function is used to select the action with max values \n",
    "    #For us to be able to select max values we need to know the state and the q_values\n",
    "    def __call__(self, state, q_values): \n",
    "\n",
    "        is_greedy = random.random() > self.epsilon\n",
    "\n",
    "        if is_greedy:\n",
    "            # we select a greedy action by getting the max q_values from the grid\n",
    "            action_index = np.argmax(q_values[state])\n",
    "        else:\n",
    "            # else we get a random choice from action\n",
    "            action_index = random.choice(list(Action)).value.index\n",
    "        #while selected_action = None\n",
    "        selected_action = None\n",
    "        #we pick an action from our possible action moves\n",
    "        for a in list(Action):\n",
    "            if a.value.index == action_index:\n",
    "                selected_action = a\n",
    "        return selected_action\n",
    "\n",
    "    #TODO understand what this is doing\n",
    "    def update_epsilon(self):\n",
    "        self.epsilon = self.epsilon * self.decay\n",
    "\n",
    "    #TODO do we need this?\n",
    "    def reset(self):\n",
    "        self.epsilon = self.epsilon_start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Qlearning:\n",
    "    \"\"\"Instant diff parameters for calc Q\"\"\"\n",
    "    def __init__(self, policy, env, gamma, alpha):\n",
    "        self.policy = policy\n",
    "        self.gamma = gamma\n",
    "        self.alpha = alpha\n",
    "\n",
    "        self.env = env.size\n",
    "        self.coord_to_index_state = env.coord_to_index_state\n",
    "\n",
    "        self.q_values = np.zeros( (self.env * self.env,(len(list(Action) ))))\n",
    "\n",
    "\n",
    "    #We are updating the values from our q.values table after each step\n",
    "    def update_values(self, s_current, action_next, r_next, s_next, action_next_next):\n",
    "\n",
    "        self.q_values[s_current, action_next] = self.q_values[s_current, action_next] + self.alpha * (\n",
    "        r_next+ self.gamma * self.q_values[s_next, action_next_next]- self.q_values)\n",
    "\n",
    "\n",
    "\n",
    "    #we are assigning the maximum qvalues to the grid in the maze so we can calculate the optimum route the agent must take in order to maximise reward\n",
    "    def new_values(self):\n",
    "\n",
    "        value_grid = np.zeros((self.env, self.env))\n",
    "\n",
    "        for i in range(self.env):\n",
    "            for j in range(self.env):\n",
    "\n",
    "                s = self.coord_to_index_state[i, j]\n",
    "\n",
    "                value_grid[i, j] = max(self.q_values[s])\n",
    "\n",
    "        return value_grid\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "X X X X X X X X X X X X X X X X X X X X \nX X . X . X X . A . . . X . X X . X . X \nX . . . . . . . X X X X X . X X . . . X \nX . X X X X X . X X . . . . . X . X X X \nX X X X X X X . X . . X X X X X . . . X \nX X . . . . . . X . X X . X . X . X X X \nX . . X X X X . X . . . . . . X . X . X \nX . X X . . . . . . X X . X . . . . . X \nX X X X X X X . X X X . . X . X . X . X \nX X . X X . . . . . X X X X . X . X . X \nX . . . X X X . X X X . X X X X X X . X \nX X X . X . . . . X X . . . X . X X . X \nX . . . X X X X . X . . X . . . X X . X \nX . X . . X X X . . . X X X X X X . . X \nX . X X . . . . . X . . X X . . X X . X \nX . . X X . X . X X X . . . . X X . . X \nX . X X X . X . . X X . X X X X X X X X \nX X X . X X X X . . X . . . X . X . . X \nX . . . . . . . . X X . X . . . . . X X \nX X X X X X X X X X X X X X X X X O X X \n\n"
     ]
    }
   ],
   "source": [
    "maze = QMaze(20)\n",
    "maze.reset()\n",
    "maze.display()"
   ]
  },
  {
   "source": [
    "## An epsilon-greedy policy\n",
    "We can combine our random policy and our greedy policy to make an improved policy that both explores its environment and exploits its current knowledge. An $\\epsilon$-greedy (epsilon-greedy) policy is one which exploits what it knows most of the time, but with probability $\\epsilon$ will instead select a random action to try.\n",
    "\n",
    "## Do we need to keep exploring once we are confident in the values of states?\n",
    "\n",
    "As our agent explores more, it becomes more confident in predicting how valuable any state is. Once it knows a lot, it should start to explore less and exploit what it knows more. That means that we should decrease epsilon over time.\n",
    "\n",
    "Let's implement it"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "epolicy = E_greedy_policy(1, 0.999)\n",
    "epolicy.reset()\n",
    "qlearning = Qlearning(epolicy,maze, 0.9, 0.1)\n",
    "\n",
    "s = maze.reset()\n",
    "done = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "ValueError",
     "evalue": "setting an array element with a sequence.",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;31mTypeError\u001b[0m: only size-1 arrays can be converted to Python scalars",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-172-1e22e5415749>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mnext_action\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mepolicy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms_next\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mqlearning\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mq_values\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m     \u001b[0mqlearning\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate_values\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mr\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0ms_next\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnext_action\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m     \u001b[0meps_policy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate_epsilon\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-169-11dd04318619>\u001b[0m in \u001b[0;36mupdate_values\u001b[0;34m(self, s_current, action_next, r_next, s_next, action_next_next)\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mupdate_values\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ms_current\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction_next\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mr_next\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ms_next\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction_next_next\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m         self.q_values[s_current, action_next] = self.q_values[s_current, action_next] + self.alpha * (\n\u001b[0m\u001b[1;32m     18\u001b[0m         r_next+ self.gamma * self.q_values[s_next, action_next_next]- self.q_values)\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: setting an array element with a sequence."
     ]
    }
   ],
   "source": [
    "#we are trying to use epsilon after each step \n",
    "turns_elapsed = []\n",
    "while not done:\n",
    "    action = epolicy(s,qlearning.q_values)\n",
    "    s_next,r, done = maze.step(action)\n",
    "    next_action = epolicy(s_next, qlearning.q_values)\n",
    "\n",
    "    qlearning.update_values(s, action.value.index,r,s_next,next_action.value.index)\n",
    "    eps_policy.update_epsilon()\n",
    "\n",
    "    s = s_next\n",
    "   # self, s_current, action_next, r_next, s_next, action_next_next\n",
    "\n",
    "\n",
    "   values = qlearning.new_values()\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}